{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## GOAL: Prediction of setter behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "import dvwtools.read as dv\n",
    "import dvwtools.stats as dvstats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from IPython.core.display import display\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import helper_classification as helpclass\n",
    "\n",
    "importlib.reload(dvstats)\n",
    "importlib.reload(dv)\n",
    "importlib.reload(helpclass)\n",
    "\n",
    "pd.set_option(\"max_columns\", None)\n",
    "pd.set_option(\"mode.chained_assignment\", \"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loads the image of the volleyball court\n",
    "path_image = os.path.join(\"assets\", \"mezzoCampo_up.png\")\n",
    "path_image_set = os.path.join(\"assets\", \"mezzoCampo.png\")\n",
    "\n",
    "# Just some handling of the volleyball court image\n",
    "encoded_image_up = base64.b64encode(open(path_image, \"rb\").read())\n",
    "encoded_image_down = base64.b64encode(open(path_image_set, \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TUNE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Predicting setter distribution in high-level volleyball teams<span class=\"tocSkip\"></span></h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "December 2021\n",
    "\n",
    "Andrea Biasioli\n",
    "\n",
    "Aknowledgments: I would like to thank César Hernández González (Head Coach of Korea NT and Assistant Coach of Vakifbank Istanbul) for the data used in this work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Main objective\n",
    "Notion of side-out: in a team, the setter is responsible for the second ball touch (set), occurring after the first touch (reception), delivering the ball to one of the attackers for the third and last allowed touch (attack). See image below.\n",
    "\n",
    "While the choice might seem (and sometimes is) random, most of the time there is a reasoning behind the setter's choice. It might be related to the height of the different blockers, or the presence of a strong attacker in the lineup, other factors, or none of the above.\n",
    "\n",
    "**The goal of the model is to *predict* the setter choice in side-out**, with the available information. More specifically, this study targets the Italian club team **Imoco Conegliano** and its setter **Asia Wolosz**; Conegliano currently holds the world record for most consecutive wins (76) and competing in the 2021 Volleyball Club World Championship.\n",
    "\n",
    "The importance of the analysis is related to **limiting the opponent team side-out efficiency by learning in advance what are the patterns that are more likely to repeat** during the game.\n",
    "\n",
    "The implications of this study are:\n",
    "- **if a simple and explainable model, like a decision tree, can be developed, then the decision tree itself could be printed out for players to aid their decisions** in blocking. A reasonable goal for the model would be to help limit the competence area that a middle blocker should be able to cover. The assumption is that if a middle blocker competence area is only 4.5m long (instead of the full 9m of the net) she might have a better chance of success in controlling the opponent attack.\n",
    "- **if a complex model is needed, then it could be implemented in a live system using the laptop on the bench for real-time predictions**. The model would predict the setter choice right before the side-out occurs, using the  information available beforehand.\n",
    "- the study of patterns is often done manually, by analyzing hours and hours of video, and it mostly follows a pattern of its own. For example, divide the rallies by setter rotation, subdivide by setter calls, and look at occurrence frequencies. While this analysis is largely successful and unbiased, it can fail to capture larger dynamics (in the example above, dynamics that extend to multiple rotations or calls) because of the high variance of the highly specific video analysis. This study can help by providing both a sanity-check benchmark for the video analysis (**verify that the results from automated and manual system indeed match**), and could also **highlight what the most important factors are without any prior knowledge of the setter**. For example, the most important factor for one setter might not be the specific setter call, but the height of the different blockers instead, while for another setter it could be uniquely the location of the reception.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img alt=\"sideout\" src=\"images/cumulated.jpg\" width=\"850\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Predicting-set-distribution-in-volleyball\" data-toc-modified-id=\"Predicting-set-distribution-in-volleyball-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Predicting set distribution in volleyball</a></span></li><li><span><a href=\"#Main-objective\" data-toc-modified-id=\"Main-objective-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Main objective</a></span></li><li><span><a href=\"#Introduction-to-the-dataset\" data-toc-modified-id=\"Introduction-to-the-dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Introduction to the dataset</a></span></li><li><span><a href=\"#Data-cleaning\" data-toc-modified-id=\"Data-cleaning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Descriptive-statistics\" data-toc-modified-id=\"Descriptive-statistics-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Descriptive statistics</a></span></li><li><span><a href=\"#set_x\" data-toc-modified-id=\"set_x-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>set_x</a></span></li><li><span><a href=\"#set_y\" data-toc-modified-id=\"set_y-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>set_y</a></span></li><li><span><a href=\"#reception_x\" data-toc-modified-id=\"reception_x-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>reception_x</a></span></li><li><span><a href=\"#reception_y\" data-toc-modified-id=\"reception_y-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>reception_y</a></span></li><li><span><a href=\"#attack_x\" data-toc-modified-id=\"attack_x-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>attack_x</a></span></li><li><span><a href=\"#serve_x\" data-toc-modified-id=\"serve_x-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>serve_x</a></span></li><li><span><a href=\"#ScoreScaled\" data-toc-modified-id=\"ScoreScaled-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>ScoreScaled</a></span></li><li><span><a href=\"#ScoreDelta\" data-toc-modified-id=\"ScoreDelta-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>ScoreDelta</a></span></li><li><span><a href=\"#sideout_no\" data-toc-modified-id=\"sideout_no-4.10\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;</span>sideout_no</a></span></li><li><span><a href=\"#block-heights\" data-toc-modified-id=\"block-heights-4.11\"><span class=\"toc-item-num\">4.11&nbsp;&nbsp;</span>block heights</a></span></li><li><span><a href=\"#call\" data-toc-modified-id=\"call-4.12\"><span class=\"toc-item-num\">4.12&nbsp;&nbsp;</span>call</a></span></li><li><span><a href=\"#players-codes\" data-toc-modified-id=\"players-codes-4.13\"><span class=\"toc-item-num\">4.13&nbsp;&nbsp;</span>players codes</a></span></li><li><span><a href=\"#IsSetterBlocking\" data-toc-modified-id=\"IsSetterBlocking-4.14\"><span class=\"toc-item-num\">4.14&nbsp;&nbsp;</span>IsSetterBlocking</a></span></li></ul></li><li><span><a href=\"#Data-preparation-and-feature-engineering\" data-toc-modified-id=\"Data-preparation-and-feature-engineering-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data preparation and feature engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#serve_x\" data-toc-modified-id=\"serve_x-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>serve_x</a></span></li><li><span><a href=\"#sideout_no\" data-toc-modified-id=\"sideout_no-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>sideout_no</a></span></li><li><span><a href=\"#ScoreScaled\" data-toc-modified-id=\"ScoreScaled-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>ScoreScaled</a></span></li><li><span><a href=\"#call\" data-toc-modified-id=\"call-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>call</a></span></li><li><span><a href=\"#players-codes-(MB1Code,-MB2Code,-OH1Code,-OH2Code)\" data-toc-modified-id=\"players-codes-(MB1Code,-MB2Code,-OH1Code,-OH2Code)-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>players codes (MB1Code, MB2Code, OH1Code, OH2Code)</a></span></li><li><span><a href=\"#set_y\" data-toc-modified-id=\"set_y-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>set_y</a></span></li><li><span><a href=\"#ScoreDelta\" data-toc-modified-id=\"ScoreDelta-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>ScoreDelta</a></span></li></ul></li><li><span><a href=\"#Target-variables\" data-toc-modified-id=\"Target-variables-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Target variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#attacker\" data-toc-modified-id=\"attacker-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>attacker</a></span></li><li><span><a href=\"#region_coordinate\" data-toc-modified-id=\"region_coordinate-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>region_coordinate</a></span></li><li><span><a href=\"#binarized_attacker\" data-toc-modified-id=\"binarized_attacker-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>binarized_attacker</a></span></li><li><span><a href=\"#Final-choice\" data-toc-modified-id=\"Final-choice-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Final choice</a></span></li></ul></li><li><span><a href=\"#Target:-region_combination\" data-toc-modified-id=\"Target:-region_combination-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Target: region_combination</a></span><ul class=\"toc-item\"><li><span><a href=\"#Label-encoding\" data-toc-modified-id=\"Label-encoding-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Label encoding</a></span></li><li><span><a href=\"#Preprocessing-pipeline\" data-toc-modified-id=\"Preprocessing-pipeline-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Preprocessing pipeline</a></span></li><li><span><a href=\"#Correlation-analysis\" data-toc-modified-id=\"Correlation-analysis-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Correlation analysis</a></span></li><li><span><a href=\"#Model-baseline\" data-toc-modified-id=\"Model-baseline-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Model baseline</a></span></li><li><span><a href=\"#No-tuning-results\" data-toc-modified-id=\"No-tuning-results-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>No-tuning results</a></span></li><li><span><a href=\"#Most-important-features\" data-toc-modified-id=\"Most-important-features-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Most important features</a></span></li><li><span><a href=\"#Reduced-set-of-features\" data-toc-modified-id=\"Reduced-set-of-features-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>Reduced set of features</a></span></li><li><span><a href=\"#Random-forest-tuning\" data-toc-modified-id=\"Random-forest-tuning-7.8\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>Random forest tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Effect-of-number-of-trees\" data-toc-modified-id=\"Effect-of-number-of-trees-7.8.1\"><span class=\"toc-item-num\">7.8.1&nbsp;&nbsp;</span>Effect of number of trees</a></span></li><li><span><a href=\"#Effect-of-max_features-number\" data-toc-modified-id=\"Effect-of-max_features-number-7.8.2\"><span class=\"toc-item-num\">7.8.2&nbsp;&nbsp;</span>Effect of max_features number</a></span></li><li><span><a href=\"#Effect-of-number-of-max_leaf_nodes\" data-toc-modified-id=\"Effect-of-number-of-max_leaf_nodes-7.8.3\"><span class=\"toc-item-num\">7.8.3&nbsp;&nbsp;</span>Effect of number of max_leaf_nodes</a></span></li><li><span><a href=\"#Effect-of-min_samples_split\" data-toc-modified-id=\"Effect-of-min_samples_split-7.8.4\"><span class=\"toc-item-num\">7.8.4&nbsp;&nbsp;</span>Effect of min_samples_split</a></span></li><li><span><a href=\"#Final-random-forest-GridSearchCV-tuning\" data-toc-modified-id=\"Final-random-forest-GridSearchCV-tuning-7.8.5\"><span class=\"toc-item-num\">7.8.5&nbsp;&nbsp;</span>Final random forest GridSearchCV tuning</a></span></li><li><span><a href=\"#Random-forest-tuning-using-Hyperopt\" data-toc-modified-id=\"Random-forest-tuning-using-Hyperopt-7.8.6\"><span class=\"toc-item-num\">7.8.6&nbsp;&nbsp;</span>Random forest tuning using Hyperopt</a></span></li><li><span><a href=\"#Random-forest-tuned---reduced-model\" data-toc-modified-id=\"Random-forest-tuned---reduced-model-7.8.7\"><span class=\"toc-item-num\">7.8.7&nbsp;&nbsp;</span>Random forest tuned - reduced model</a></span></li></ul></li><li><span><a href=\"#XGBoost-tuning\" data-toc-modified-id=\"XGBoost-tuning-7.9\"><span class=\"toc-item-num\">7.9&nbsp;&nbsp;</span>XGBoost tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Max_depth\" data-toc-modified-id=\"Max_depth-7.9.1\"><span class=\"toc-item-num\">7.9.1&nbsp;&nbsp;</span>Max_depth</a></span></li><li><span><a href=\"#min_child_weight\" data-toc-modified-id=\"min_child_weight-7.9.2\"><span class=\"toc-item-num\">7.9.2&nbsp;&nbsp;</span>min_child_weight</a></span></li><li><span><a href=\"#gamma\" data-toc-modified-id=\"gamma-7.9.3\"><span class=\"toc-item-num\">7.9.3&nbsp;&nbsp;</span>gamma</a></span></li><li><span><a href=\"#XGBoost-manually-tuned-model\" data-toc-modified-id=\"XGBoost-manually-tuned-model-7.9.4\"><span class=\"toc-item-num\">7.9.4&nbsp;&nbsp;</span>XGBoost manually tuned model</a></span></li><li><span><a href=\"#XGBoost-tuning-using-hyperopt\" data-toc-modified-id=\"XGBoost-tuning-using-hyperopt-7.9.5\"><span class=\"toc-item-num\">7.9.5&nbsp;&nbsp;</span>XGBoost tuning using hyperopt</a></span></li><li><span><a href=\"#XGBoost-hyperopt-reduced-model\" data-toc-modified-id=\"XGBoost-hyperopt-reduced-model-7.9.6\"><span class=\"toc-item-num\">7.9.6&nbsp;&nbsp;</span>XGBoost hyperopt reduced model</a></span></li></ul></li><li><span><a href=\"#Is-it-possible-to-build-an-efficient-and-simplified-decision-tree-for-every-rotation_home?\" data-toc-modified-id=\"Is-it-possible-to-build-an-efficient-and-simplified-decision-tree-for-every-rotation_home?-7.10\"><span class=\"toc-item-num\">7.10&nbsp;&nbsp;</span>Is it possible to build an efficient and simplified decision tree for every <em>rotation_home</em>?</a></span></li><li><span><a href=\"#Conclusions-on-region_combination-target\" data-toc-modified-id=\"Conclusions-on-region_combination-target-7.11\"><span class=\"toc-item-num\">7.11&nbsp;&nbsp;</span>Conclusions on <em>region_combination</em> target</a></span></li></ul></li><li><span><a href=\"#Target:-binary_attacker\" data-toc-modified-id=\"Target:-binary_attacker-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Target: binary_attacker</a></span><ul class=\"toc-item\"><li><span><a href=\"#Label-encoding\" data-toc-modified-id=\"Label-encoding-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Label encoding</a></span></li><li><span><a href=\"#Preprocessing-pipeline\" data-toc-modified-id=\"Preprocessing-pipeline-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Preprocessing pipeline</a></span></li><li><span><a href=\"#No-tuning-results\" data-toc-modified-id=\"No-tuning-results-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>No-tuning results</a></span></li><li><span><a href=\"#No-tuning-results-(Reduced-model)\" data-toc-modified-id=\"No-tuning-results-(Reduced-model)-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>No-tuning results (Reduced model)</a></span></li><li><span><a href=\"#XGBoost-binary-tuning-on-full-set\" data-toc-modified-id=\"XGBoost-binary-tuning-on-full-set-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>XGBoost binary tuning on full set</a></span><ul class=\"toc-item\"><li><span><a href=\"#XGBoost-binary-tuning-on-reduced-features-set\" data-toc-modified-id=\"XGBoost-binary-tuning-on-reduced-features-set-8.5.1\"><span class=\"toc-item-num\">8.5.1&nbsp;&nbsp;</span>XGBoost binary tuning on reduced features set</a></span></li></ul></li><li><span><a href=\"#SVC-tuning-on-full-set\" data-toc-modified-id=\"SVC-tuning-on-full-set-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>SVC tuning on full set</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVC-on-reduced-set\" data-toc-modified-id=\"SVC-on-reduced-set-8.6.1\"><span class=\"toc-item-num\">8.6.1&nbsp;&nbsp;</span>SVC on reduced set</a></span></li></ul></li><li><span><a href=\"#Logistic-regression-tuning-on-full-set\" data-toc-modified-id=\"Logistic-regression-tuning-on-full-set-8.7\"><span class=\"toc-item-num\">8.7&nbsp;&nbsp;</span>Logistic regression tuning on full set</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-regression-on-reduced-set\" data-toc-modified-id=\"Logistic-regression-on-reduced-set-8.7.1\"><span class=\"toc-item-num\">8.7.1&nbsp;&nbsp;</span>Logistic regression on reduced set</a></span></li></ul></li><li><span><a href=\"#K-Neighbors-tuning-on-full-set\" data-toc-modified-id=\"K-Neighbors-tuning-on-full-set-8.8\"><span class=\"toc-item-num\">8.8&nbsp;&nbsp;</span>K-Neighbors tuning on full set</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Neighbors-on-reduced-set\" data-toc-modified-id=\"K-Neighbors-on-reduced-set-8.8.1\"><span class=\"toc-item-num\">8.8.1&nbsp;&nbsp;</span>K-Neighbors on reduced set</a></span></li></ul></li><li><span><a href=\"#Ensamble-results\" data-toc-modified-id=\"Ensamble-results-8.9\"><span class=\"toc-item-num\">8.9&nbsp;&nbsp;</span>Ensamble results</a></span></li><li><span><a href=\"#Conclusions-on-binary_attacker-target\" data-toc-modified-id=\"Conclusions-on-binary_attacker-target-8.10\"><span class=\"toc-item-num\">8.10&nbsp;&nbsp;</span>Conclusions on <em>binary_attacker</em> target</a></span></li></ul></li><li><span><a href=\"#Model-interpretation\" data-toc-modified-id=\"Model-interpretation-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Model interpretation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Global-interpretability\" data-toc-modified-id=\"Global-interpretability-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Global interpretability</a></span><ul class=\"toc-item\"><li><span><a href=\"#Left-(L)\" data-toc-modified-id=\"Left-(L)-9.1.1\"><span class=\"toc-item-num\">9.1.1&nbsp;&nbsp;</span>Left (L)</a></span></li><li><span><a href=\"#Right-(R)\" data-toc-modified-id=\"Right-(R)-9.1.2\"><span class=\"toc-item-num\">9.1.2&nbsp;&nbsp;</span>Right (R)</a></span></li><li><span><a href=\"#Middle-(M)\" data-toc-modified-id=\"Middle-(M)-9.1.3\"><span class=\"toc-item-num\">9.1.3&nbsp;&nbsp;</span>Middle (M)</a></span></li></ul></li><li><span><a href=\"#Partial-dependence-plots\" data-toc-modified-id=\"Partial-dependence-plots-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Partial dependence plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importance-of-set_x-in-L/R-decision\" data-toc-modified-id=\"Importance-of-set_x-in-L/R-decision-9.2.1\"><span class=\"toc-item-num\">9.2.1&nbsp;&nbsp;</span>Importance of <em>set_x</em> in L/R decision</a></span></li><li><span><a href=\"#Importance-of-set_y-in-M-decisions\" data-toc-modified-id=\"Importance-of-set_y-in-M-decisions-9.2.2\"><span class=\"toc-item-num\">9.2.2&nbsp;&nbsp;</span>Importance of <em>set_y</em> in M decisions</a></span></li></ul></li><li><span><a href=\"#Local-predictability:-single-side-out-prediction\" data-toc-modified-id=\"Local-predictability:-single-side-out-prediction-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Local predictability: single side-out prediction</a></span></li></ul></li><li><span><a href=\"#General-conclusions\" data-toc-modified-id=\"General-conclusions-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>General conclusions</a></span></li><li><span><a href=\"#Suggestions-and-next-steps\" data-toc-modified-id=\"Suggestions-and-next-steps-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Suggestions and next steps</a></span></li></ul></div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction to the dataset\n",
    "\n",
    "The dataset was generated using DataVolley DVW files, **kindly provided by César Hernández González (Head Coach of South Korea W's NT and Assistant Coach of Vakifbank Istanbul)**.\n",
    "\n",
    "A look into five rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_sideout = pd.read_parquet(\"datasets/conegliano/gold_conegliano.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"game_name\",\n",
    "    \"opponent_team\",\n",
    "    \"rotation_guest\",\n",
    "    \"rotation_home\",\n",
    "    \"current_game\",\n",
    "    \"ScoreScaled\",\n",
    "    \"ScoreDelta\",\n",
    "    \"reception_quality\",\n",
    "    \"reception_x\",\n",
    "    \"reception_y\",\n",
    "    \"serve_x\",\n",
    "    \"call\",\n",
    "    \"set_x\",\n",
    "    \"set_y\",\n",
    "    \"attack_x\",\n",
    "    \"sideout_no\",\n",
    "    \"F_Block_height\",\n",
    "    \"C_Block_height\",\n",
    "    \"B_Block_height\",\n",
    "    \"attack_quality\",\n",
    "    \"SetCode\",\n",
    "    \"OppCode\",\n",
    "    \"OH1Code\",\n",
    "    \"OH2Code\",\n",
    "    \"MB1Code\",\n",
    "    \"MB2Code\",\n",
    "    \"IsSetterBlocking\",\n",
    "    \"region_coordinate\",\n",
    "    \"attacker\",\n",
    "]\n",
    "\n",
    "df_sideout = df_sideout[cols]\n",
    "df_sideout.game_name = df_sideout.game_name.astype(str)\n",
    "df_sideout.opponent_team = df_sideout.opponent_team.astype(str)\n",
    "display(df_sideout.sample(5))\n",
    "print(\"\\n\")\n",
    "print(df_sideout.info())\n",
    "df_copy = df_sideout.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each row represents a side-out occurrence and its info.\n",
    "\n",
    "Let's identify features that are *NOT* useful for the analysis:\n",
    "- *game_name* (str): unlikely to make a difference, it's just a nice reference if we want to analyze only specific competitions\n",
    "- *opponent_team* (str): as before, if we want to analyze only a specific opponent\n",
    "- *current_game* (int): also called set, discrete values from 1 to 5, it is unlikely to provide useful info\n",
    "\n",
    "Useful features:\n",
    "- *rotation_guest* (int): Discrete values from 1 to 6. Mostly useful to distinguish rotations with the guest setter blocking or guest opposite blocking\n",
    "- *rotation_home* (str): Discrete values from 1 to 6. The order of occurrence is always 1-6-5-4-3-2, because of the rule of the volleyball sport. It is encoded as string because sklearn cannot process not-ordered numerical in its LabelEncoder transformer.\n",
    "- *ScoreScaled* (int): volleyball sets/games end at 25, except the tie-break (5th sets) that ends at 15. Here, the scores in the tie-breaks are multiplied by 1.66 for scaling, so the \"heat\" of the game is represented somewhat equally for regular sets and tie-breaks\n",
    "- *ScoreDelta* (int): different in score between Conegliano and the opponent team\n",
    "- *reception_quality* (char): indication of the quality of the reception. This analysis only includes '#' and '+' passes (the ones of better quality)\n",
    "- *reception_x* (int): x-coordinate of the reception on the court\n",
    "- *reception_y* (int): y-coordinate of the reception on the court\n",
    "- *serve_x* (int): x-coordinate of the serve\n",
    "- *call* (str): Setter call for the middle blocker\n",
    "- *set_x* (float): x-coordinate of the setter on the court when setting\n",
    "- *set_y* (float): y-coordinate of the setter on the court when setting\n",
    "- *attack_x* (float): x-coordinate of the attacker when hitting the ball\n",
    "- *sideout_no* (int): number of consecutive side-out occurrences. For example, if equal to 2 it means that Conegliano failed a 1st attempt, and this is the 2nd attempt in a row\n",
    "- *F_Block_height* (int): block height of the front blocker (usually opposite/setter) in cm\n",
    "- *B_Block_height* (int): block height of the back blocker (usually outside hitter) in cm\n",
    "- *C_Block_height* (int): block height of the center blocker (usually middle blocker) in cm\n",
    "- *IsSetterBlocking* (bool): tells if the opponent is in rotation 2, 3, or 4. It renders the *rotation_guest* attribute less useful\n",
    "- *attack_quality* (char): outcome of the side-out attack\n",
    "- *SetCode* (str): code related to the setter present in the court in the side-out rally\n",
    "- *OppCode* (str): code related to the opposite present in the court in the side-out rally\n",
    "- *OH1Code* (str): code related to the outside hitter 1 present in the court in the side-out rally\n",
    "- *OH2Code* (str): code related to the outside hitter 2 present in the court in the side-out rally\n",
    "- *MB1Code* (str): code related to the middle blocker 1 present in the court in the side-out rally\n",
    "- *MB2Code* (str): code related to the middle blocker 2 present in the court in the side-out rally\n",
    "\n",
    "Tentative **target labels**:\n",
    "- *attacker* (char): identifies the attacker in the rally\n",
    "- *region_coordinate* (char): identifies the area of the net where the attack occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data cleaning\n",
    "\n",
    "Let's start by looking for missing/illegal values, the distribution of the different features, and the kind of information they contain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(df_sideout.drop(columns=[\"rotation_guest\", \"current_game\"]).describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dataset contains 1521 rows.\n",
    "\n",
    "Values of -1 for *set_x, set_y* are illegal, indicating missing info.\n",
    "As this is a relatively small dataset, it is important to carefully inspect all features (luckily they are not too many) to make sure we do not have many outliers or missing values that can alter the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## set_x\n",
    "This feature only has 1 missing values (0.06%). We will impute them with the median of the feature during the preprocessing. It looks normally distributed, so it will not need to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sideout[df_sideout.set_x == -1].game_name.value_counts())\n",
    "df_sideout.set_x.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## set_y\n",
    "As for *set_x*, 0.06% of the values are missing and will be imputed. The variable is left-skewed and will need to be transformed with a PowerTransformer or a QuantileTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sideout[df_sideout.set_y == -1].game_name.value_counts())\n",
    "df_sideout.set_y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## reception_x\n",
    "No missing values, the distribution does not have a clear shape, but an almost uniform profile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sideout[df_sideout.reception_x == -1].game_name.value_counts())\n",
    "df_sideout.reception_x.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## reception_y\n",
    "No missing values, it appears left-skewed. It needs a PowerTransformer or QuantileTransformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sideout[df_sideout.reception_y == -1].game_name.value_counts())\n",
    "df_sideout.reception_y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## attack_x\n",
    "No missing values, bimodal distribution. This variable can be binned and used as target label for the setter distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sideout[df_sideout.attack_x == -1].game_name.value_counts())\n",
    "df_sideout.attack_x.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## serve_x\n",
    "No missing values, bimodal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sideout[df_sideout.serve_x == -1].game_name.value_counts())\n",
    "df_sideout.serve_x.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ScoreScaled\n",
    "No missing values, almost uniform distribution with a small right tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sideout[df_sideout.ScoreScaled == -1].game_name.value_counts())\n",
    "df_sideout.ScoreScaled.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ScoreDelta\n",
    "No missing values, variable with a light right-skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sideout[df_sideout.ScoreDelta.isna()].game_name.value_counts())\n",
    "df_sideout.ScoreDelta.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## sideout_no\n",
    "The variable is very unbalanced, with about 89% of the occurrences being a first side-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sideout[df_sideout.sideout_no.isna()].game_name.value_counts())\n",
    "df_sideout.sideout_no.hist()\n",
    "print(df_sideout.sideout_no.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## block heights\n",
    "We can notice that *F_Block_height* is the one with the most variability, as often there is a noticeable height difference between opposites and setters (the players blocking on the 'F' side). No missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(\n",
    "    df_sideout,\n",
    "    x=[\"F_Block_height\", \"C_Block_height\", \"B_Block_height\"],\n",
    "    nbins=15,\n",
    "    width=800,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## call\n",
    "There are two under-represented calls: KC and K2. KC can be grouped to K1, as they are functionally similar. K2 will belong to its own under-represented category, but it should not impact the results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sideout.call.value_counts(normalize=True))\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(df_sideout, x=[\"call\"], width=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img alt=\"calls\" src=\"images/ensamble_calls.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## players codes\n",
    "Setter is always Wolosz as expected, in the other positions there is a bit of alternance between players. Opposite is usually Egonu (95%), middle blocker 1 is De Krujif (78%) or Butigan (11%), middle blocker 2 is Fahr (57%) or Folie (28%), outside hitter 1 is Sylla (59%) or Mckenzie (25%) or Omoruyi (11%), finally outside hitter 2 is Hill (69%) or McKenzie (24%).\n",
    "\n",
    "Encoding for these variables can be problematic:\n",
    "- *SetCode*: it does not add any information and will be dropped\n",
    "- *OppCode*: it adds only little information, as the opposite was mostly Egonu. It will be dropped\n",
    "- *MB1Code* and *MB2Code*: I could use an OrdinalEncoder, as the cardinality is not very high. More on this later\n",
    "- *OH1Code* and *OH2Code*: same as for *MB1Code* and *MB2Code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"SetCode\", \"OppCode\", \"OH1Code\", \"OH2Code\", \"MB1Code\", \"MB2Code\"]\n",
    "df_sideout[cols] = df_sideout[cols].astype(str)\n",
    "print(f\"{df_sideout.SetCode.value_counts(normalize=True)}\\n\")\n",
    "print(f\"{df_sideout.OppCode.value_counts(normalize=True)}\\n\")\n",
    "print(f\"{df_sideout.MB1Code.value_counts(normalize=True)}\\n\")\n",
    "print(f\"{df_sideout.MB2Code.value_counts(normalize=True)}\\n\")\n",
    "print(f\"{df_sideout.OH1Code.value_counts(normalize=True)}\\n\")\n",
    "print(f\"{df_sideout.OH2Code.value_counts(normalize=True)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## IsSetterBlocking\n",
    "Balanced classes for the opponent setter blocking. No missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Null values? {df_sideout.IsSetterBlocking.isna().any()}\")\n",
    "print(df_sideout.IsSetterBlocking.value_counts(normalize=True))\n",
    "df_sideout.rotation_home = df_sideout.rotation_home.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data preparation and feature engineering\n",
    "The different features will be properly transformed, encoded, and scaled. Here, the treatment for each feature is explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df_sideout.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## serve_x\n",
    "The variable can be binarized in left-side serve (0) and right-side serve (1) to simplify the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.serve_x = np.where(df_clean.serve_x > 50, 1, 0)\n",
    "df_clean.serve_x.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## sideout_no\n",
    "I will substitute this feature with a **new binary feature *is_first_sideout***, equal to 1 for True, and 0 for False, hence coalescing all the occurrences of sideout_no > 1 in  *is_first_sideout = False*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean.assign(is_first_sideout=np.where(df_clean.sideout_no == 1, 1, 0))\n",
    "print(df_clean.is_first_sideout.value_counts(normalize=True))\n",
    "df_clean.is_first_sideout.hist()\n",
    "df_clean = df_clean.drop(columns=\"sideout_no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ScoreScaled\n",
    "I will coalesce all the right-side tail values to 25 (the end of a set is always an important moment, whether the set ends at 25 or 35)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.loc[df_clean.ScoreScaled > 25, \"ScoreScaled\"] = 25\n",
    "df_clean.ScoreScaled.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## call\n",
    "Coalesce KC into K1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.call = df_clean.call.replace({\"KC\": \"K1\", \"KB\": \"KF\"})\n",
    "calls_classes = [\"K7\", \"K1\", \"K2\", \"KF\"]\n",
    "print(df_clean.call.value_counts(normalize=True))\n",
    "print(f\"Setter classes: {calls_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## players codes (MB1Code, MB2Code, OH1Code, OH2Code)\n",
    "For each feature, extract the classes with more than 10% occurrence rate. Use OrdinalEncoder to encode those classes, and coalesce the rest of the unrepresented classes in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.10\n",
    "\n",
    "dt = df_clean.OH1Code.value_counts(normalize=True)\n",
    "OH1_classes = dt.index.values[dt > threshold].tolist()\n",
    "\n",
    "dt = df_clean.OH2Code.value_counts(normalize=True)\n",
    "OH2_classes = dt.index.values[dt > threshold].tolist()\n",
    "\n",
    "dt = df_clean.MB1Code.value_counts(normalize=True)\n",
    "MB1_classes = dt.index.values[dt > threshold].tolist()\n",
    "\n",
    "dt = df_clean.MB2Code.value_counts(normalize=True)\n",
    "MB2_classes = dt.index.values[dt > threshold].tolist()\n",
    "\n",
    "print(f\"OH1 classes: {OH1_classes}\")\n",
    "print(f\"OH2 classes: {OH2_classes}\")\n",
    "print(f\"MB1 classes: {MB1_classes}\")\n",
    "print(f\"MB2 classes: {MB2_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## set_y\n",
    "Use a PowerTransformer for this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "helpclass.display_transformations(df_clean, \"set_y\", show_normal_test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ScoreDelta\n",
    "Use a PowerTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "helpclass.display_transformations(df_clean, \"ScoreDelta\", show_normal_test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Target variables\n",
    "The goal is to predict who the setter will set to. Some possible variables to look at are:\n",
    "- *attacker*\n",
    "- *region_coordinate*\n",
    "- *binarized_attack*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## attacker\n",
    "This can be the best possible label for the analysis, as it identifies the attacker that receives the ball: F for front, C for center, B for back, P for pipe, S for setter. However, these are many classes to be predicted with only little data available (about 1500 rows). Moreover, some of the classes (P, S) are under-represented, leading to an unbalanced classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.attacker.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![attacker](images/attacker.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## region_coordinate\n",
    "Based on the x-coordinate of the attack (*attack_x*), the attacks have been encoded in only three categories: L for left, M for middle, R for right, identifying which area of the net is targeted by the setter. These labels are much more balanced, and we have only two labels to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean.region_coordinate.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![region_ccordinate](images/region_coordinate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## binarized_attacker\n",
    "This last target is the simpler:\n",
    "- 1: the setter is setting in front of her (i.e., *attack_x* >= *set_x*)\n",
    "- 0: the setter is setting behind her (*attack_x* < *set_x*)\n",
    "\n",
    "This label is moderately unbalanced, but it could be simpler to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean.assign(\n",
    "    binarized_attacker=np.select(\n",
    "        [df_clean.set_x >= df_clean.attack_x, df_clean.set_x < df_clean.attack_x],\n",
    "        [0, 1],\n",
    "        default=-1,\n",
    "    )\n",
    ")\n",
    "print(df_clean.binarized_attacker.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![binarized_attacker](images/binary_attacker.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Final choice\n",
    "Two target labels will be tested: first we will try to predict the set with *region_coordinate*. After that, we will see if the result can be improved by simplifying the classification task to a binary task with the *binarized_attack* label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Target: region_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target = \"region_coordinate\"\n",
    "df_clean = df_clean.dropna(subset=[target])\n",
    "print(df_clean[target].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"F_Block_height\",\n",
    "    \"C_Block_height\",\n",
    "    \"B_Block_height\",\n",
    "    \"ScoreDelta\",\n",
    "    \"ScoreScaled\",\n",
    "    \"reception_x\",\n",
    "    \"reception_y\",\n",
    "]\n",
    "df_clean[cols] = df_clean[cols].astype(float)\n",
    "df_clean[[\"serve_x\", \"is_first_sideout\"]] = df_clean[\n",
    "    [\"serve_x\", \"is_first_sideout\"]\n",
    "].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Label encoding\n",
    "Using StratifiedShuffleSplit, with n_splits=1, with 75%-25% train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "target = \"region_coordinate\"\n",
    "df_temp = df_clean.copy()\n",
    "df_temp.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y = df_temp[target]\n",
    "X = df_temp.drop(columns=target)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.25)\n",
    "train_index, test_index = next(sss.split(X, y))\n",
    "\n",
    "X_train = X.iloc[train_index]\n",
    "y_train = y.iloc[train_index]\n",
    "X_test = X.iloc[test_index]\n",
    "y_test = y.iloc[test_index]\n",
    "\n",
    "# Training set\n",
    "enc_target = LabelEncoder()\n",
    "y_train = enc_target.fit_transform(y_train)\n",
    "y_test = enc_target.transform(y_test)\n",
    "\n",
    "print(f\"Encoder Classes: {enc_target.classes_}\")\n",
    "print(\"\\nFrequency in stratified shuffle train split:\")\n",
    "print(pd.Series(enc_target.inverse_transform(y_train)).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<code>\n",
    "SetLocationTransformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=-1, strategy='median')),\n",
    "        (\"pt\", PowerTransformer(standardize=True)),\n",
    "])\n",
    "\n",
    "ReceptionTransformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=-1, strategy='median')),\n",
    "])\n",
    "\n",
    "HeightTransformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy='median')),\n",
    "])\n",
    "\n",
    "BinaryTransformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"enc\", OrdinalEncoder(categories=[[False, True]]))\n",
    "])\n",
    "\n",
    "SetterCallEncoder = Pipeline([(\"enc\", OrdinalEncoder(categories=[calls_classes], handle_unknown='use_encoded_value', unknown_value=len(calls_classes)))])\n",
    "\n",
    "TransformerFull = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ordinal_enc\", OrdinalEncoder(categories=[['2', '3', '4', '5', '6', '1']]), ['rotation_home']),\n",
    "        (\"call_enc\", SetterCallEncoder, [\"call\"]),\n",
    "        (\"height_encoder\", HeightTransformer, ['F_Block_height', 'B_Block_height', 'C_Block_height']),\n",
    "        (\"reception_loc\", ReceptionTransformer, ['reception_x', 'reception_y']),\n",
    "        (\"set_location_enc\", SetLocationTransformer, ['set_x', 'set_y']),\n",
    "        (\"binary_imputer\", OrdinalEncoder(), ['IsSetterBlocking', 'serve_x', 'is_first_sideout']),\n",
    "        (\"OH1_transformer\", OrdinalEncoder(categories=[OH1_classes], handle_unknown='use_encoded_value', unknown_value=len(OH1_classes)), ['OH1Code']),\n",
    "        (\"OH2_transformer\", OrdinalEncoder(categories=[OH2_classes], handle_unknown='use_encoded_value', unknown_value=len(OH2_classes)), ['OH2Code']),\n",
    "        (\"MB1_transformer\", OrdinalEncoder(categories=[MB1_classes], handle_unknown='use_encoded_value', unknown_value=len(MB1_classes)), ['MB1Code']),\n",
    "        (\"MB2_transformer\", OrdinalEncoder(categories=[MB2_classes], handle_unknown='use_encoded_value', unknown_value=len(MB2_classes)), ['MB2Code']),\n",
    "        (\"delta_transformer\", PowerTransformer(standardize=True), ['ScoreDelta']),\n",
    "        (\"passthrough\", \"passthrough\", ['ScoreScaled'])\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "preprocessor_full = Pipeline(\n",
    "    [\n",
    "        ('transformer', TransformerFull),\n",
    "        ('scaler', RobustScaler())\n",
    "    ]\n",
    ")\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "SetLocationTransformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=-1, strategy=\"median\")),\n",
    "        (\"pt\", PowerTransformer(standardize=True)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "ReceptionTransformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(missing_values=-1, strategy=\"median\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "HeightTransformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "BinaryTransformer = Pipeline(\n",
    "    steps=[(\"enc\", OrdinalEncoder(categories=[[False, True]]))]\n",
    ")\n",
    "\n",
    "SetterCallEncoder = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"enc\",\n",
    "            OrdinalEncoder(\n",
    "                categories=[calls_classes],\n",
    "                handle_unknown=\"use_encoded_value\",\n",
    "                unknown_value=len(calls_classes),\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "TransformerFull = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"ordinal_enc\",\n",
    "            OrdinalEncoder(categories=[[\"2\", \"3\", \"4\", \"5\", \"6\", \"1\"]]),\n",
    "            [\"rotation_home\"],\n",
    "        ),\n",
    "        (\"call_enc\", SetterCallEncoder, [\"call\"]),\n",
    "        (\n",
    "            \"height_encoder\",\n",
    "            HeightTransformer,\n",
    "            [\"F_Block_height\", \"B_Block_height\", \"C_Block_height\"],\n",
    "        ),\n",
    "        (\"reception_loc\", ReceptionTransformer, [\"reception_x\", \"reception_y\"]),\n",
    "        (\"set_location_enc\", SetLocationTransformer, [\"set_x\", \"set_y\"]),\n",
    "        (\n",
    "            \"binary_imputer\",\n",
    "            OrdinalEncoder(),\n",
    "            [\"IsSetterBlocking\", \"serve_x\", \"is_first_sideout\"],\n",
    "        ),\n",
    "        (\n",
    "            \"OH1_transformer\",\n",
    "            OrdinalEncoder(\n",
    "                categories=[OH1_classes],\n",
    "                handle_unknown=\"use_encoded_value\",\n",
    "                unknown_value=len(OH1_classes),\n",
    "            ),\n",
    "            [\"OH1Code\"],\n",
    "        ),\n",
    "        (\n",
    "            \"OH2_transformer\",\n",
    "            OrdinalEncoder(\n",
    "                categories=[OH2_classes],\n",
    "                handle_unknown=\"use_encoded_value\",\n",
    "                unknown_value=len(OH2_classes),\n",
    "            ),\n",
    "            [\"OH2Code\"],\n",
    "        ),\n",
    "        (\n",
    "            \"MB1_transformer\",\n",
    "            OrdinalEncoder(\n",
    "                categories=[MB1_classes],\n",
    "                handle_unknown=\"use_encoded_value\",\n",
    "                unknown_value=len(MB1_classes),\n",
    "            ),\n",
    "            [\"MB1Code\"],\n",
    "        ),\n",
    "        (\n",
    "            \"MB2_transformer\",\n",
    "            OrdinalEncoder(\n",
    "                categories=[MB2_classes],\n",
    "                handle_unknown=\"use_encoded_value\",\n",
    "                unknown_value=len(MB2_classes),\n",
    "            ),\n",
    "            [\"MB2Code\"],\n",
    "        ),\n",
    "        (\"delta_transformer\", PowerTransformer(standardize=True), [\"ScoreDelta\"]),\n",
    "        (\"passthrough\", \"passthrough\", [\"ScoreScaled\"]),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "preprocessor_full = Pipeline(\n",
    "    [(\"transformer\", TransformerFull), (\"scaler\", RobustScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Correlation analysis\n",
    "As *IsSetterBlocking* is correlated with *F_Block_height*, it is preferable to exclude one of the two variables, as they are likely providing the same info twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_features_from_transformer_list(transformers_list):\n",
    "    features_transformer = []\n",
    "    for transformer in transformers_list.transformers:\n",
    "        features_transformer = features_transformer + transformer[2]\n",
    "    return features_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from seaborn import heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.set_theme(style=\"white\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "features = get_features_from_transformer_list(TransformerFull)\n",
    "# print(f'Features: {features}')\n",
    "\n",
    "pp = preprocessor_full\n",
    "test_df = pd.DataFrame(data=preprocessor_full.fit_transform(X))\n",
    "test_df.columns = features\n",
    "corr = test_df.corr()\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "cmap = sns.diverging_palette(220, 0)\n",
    "heatmap(corr, cmap=\"coolwarm\", center=0, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs = corr.abs().unstack()\n",
    "pairs = pairs[pairs < 1]\n",
    "pairs = pairs.sort_values(kind=\"quicksort\", ascending=False)\n",
    "print(\"Most correlated pairs:\")\n",
    "display(pairs.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove IsSetterBlocking from pipeline\n",
    "preprocessor_full.named_steps[\"transformer\"].transformers[5] = (\n",
    "    \"binary_imputer\",\n",
    "    OrdinalEncoder(),\n",
    "    [\"serve_x\", \"is_first_sideout\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sklearn.set_config(None, None, None, \"diagram\")\n",
    "preprocessor_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Models definition\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load\n",
    "\n",
    "DT = DecisionTreeClassifier(random_state=42)\n",
    "RF = RandomForestClassifier(\n",
    "    oob_score=True, random_state=42, warm_start=True, n_jobs=-1, bootstrap=True\n",
    ")\n",
    "ET = ExtraTreesClassifier(oob_score=True, bootstrap=True, random_state=42, n_jobs=-1)\n",
    "lr = LogisticRegression(\n",
    "    solver=\"saga\", max_iter=2000, tol=1e-2, dual=False, random_state=42\n",
    ")\n",
    "kn = KNeighborsClassifier(n_jobs=-1)\n",
    "lsvc = LinearSVC(random_state=42, tol=1e-2, max_iter=4000, dual=False)\n",
    "svc = SVC(random_state=42, tol=1e-2, max_iter=4000, cache_size=1000)\n",
    "gb = GradientBoostingClassifier(tol=1e-2, random_state=42)\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "xgbm = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "hgb = HistGradientBoostingClassifier(random_state=42, tol=1e-2)\n",
    "nb = CategoricalNB()\n",
    "\n",
    "classifiers = [DT, RF, ET, lr, kn, lsvc, svc, gb, ada, xgbm, hgb]\n",
    "names = [\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Extra Trees\",\n",
    "    \"Logistic Regression\",\n",
    "    \"K-Neighbors\",\n",
    "    \"Linear SVC\",\n",
    "    \"SVC\",\n",
    "    \"Gradient Boosting\",\n",
    "    \"ADABoost\",\n",
    "    \"XGBoost\",\n",
    "    \"HistGradientBoosting\",\n",
    "]\n",
    "save_name = [\"dt\", \"rf\", \"et\", \"lr\", \"kn\", \"lsvc\", \"svc\", \"gb\", \"ada\", \"xgb\", \"hgb\"]\n",
    "# params = [DT_params, RF_params, ET_params, lr_params, kn_params, lsvc_params, svc_params, gb_params, ada_params, xgb_params]\n",
    "parameters = [{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model baseline\n",
    "The baseline is given by assuming the majority class is always the answer. In this case, assuming that the setter choice will always be 0 (i.e., Left side of the net), we would be approximately 36% accurate. **If our model is able to predict with more than 36% accuracy, then we gained some predictive power**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Classes: {enc_target.classes_}\")\n",
    "pd.Series(y_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## No-tuning results\n",
    "Let's compare results from different classification models. **We want the model to beat the baseline (accuracy > 36%)**.\n",
    "\n",
    "As the dataset is small, with a fairly small amount of features (18), I expect simpler models to give the best results. We notice however that the accuracy performance on the training set is 1, while the one on the test set is not: the large difference suggests over-fitting is occurring. We will need to better adjust our models to improve the generalized predictive power (in this case measured with the performance on the test set).\n",
    "\n",
    "**Out-of-the-box solutions provide values of about 47% accuracy (11% better than the baseline)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(helpclass)\n",
    "folder = \"models/region_coordinate/no_tuning/\"\n",
    "TUNE = False\n",
    "\n",
    "if TUNE:\n",
    "    # WARNING: FIT TAKES LONG TIME!\n",
    "    log_no_tuning_df = helpclass.fit_models(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        preprocessor_full,\n",
    "        classifiers,\n",
    "        names,\n",
    "        save_name,\n",
    "        folder,\n",
    "        parameters,\n",
    "        cross_val_splits=5,\n",
    "        verbose=False,\n",
    "    )\n",
    "else:\n",
    "    log_no_tuning_df = helpclass.eval_models(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifiers,\n",
    "        names,\n",
    "        save_name,\n",
    "        folder,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(\n",
    "    log_no_tuning_df[\n",
    "        [\"model\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"accuracy_train\"]\n",
    "    ].sort_values(by=\"accuracy\", ascending=False)\n",
    ")\n",
    "helpclass.summary_mean_print(log_no_tuning_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Most important features\n",
    "As the dataset is small, the presence of many useless features can aggravate the over-fitting issue, possibly decreasing the performance of the model. I will use SHAP to identify the most important features for the model prediction. This step would be ideally repeated once models are fine-tuned, to verify the conclusions drawn from the out-of-the-box algorithm used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = get_features_from_transformer_list(TransformerFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "values = []\n",
    "aux = helpclass.shap_summary_plot_wrapper(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    load(\"models/region_coordinate/no_tuning/xgb.joblib\"),\n",
    "    features,\n",
    "    enc_target.classes_,\n",
    "    model_name=\"XGBoost\",\n",
    ")\n",
    "values.append(aux)\n",
    "aux = helpclass.shap_summary_plot_wrapper(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    load(\"models/region_coordinate/no_tuning/hgb.joblib\"),\n",
    "    features,\n",
    "    enc_target.classes_,\n",
    "    \"Hist Gradient Boosting\",\n",
    ")\n",
    "values.append(aux)\n",
    "aux = helpclass.shap_summary_plot_wrapper(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    load(\"models/region_coordinate/no_tuning/rf.joblib\"),\n",
    "    features,\n",
    "    enc_target.classes_,\n",
    "    \"Random Forest\",\n",
    ")\n",
    "values.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can point at a few features that do not seem important for any of the first four models: *is_first_sideout*, *MB1Code*, *MB2Code*, *OH1Code*, *OH2Code*. We can check how the models would perform (with no tuning) with a reduced set of features (now 11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reduced set of features\n",
    "With a reduced set of features, the results did not change significantly. The performance decreased slightly, but the models are simpler. In the next section, we will try to obtain better results by tuning the algorithms on the full set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<code>\n",
    "TransformerReduced = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"ordinal_enc\", OrdinalEncoder(categories=[['2', '3', '4', '5', '6', '1']]), ['rotation_home']),\n",
    "        (\"call_enc\", SetterCallEncoder, [\"call\"]),\n",
    "        (\"height_encoder\", HeightTransformer, ['F_Block_height', 'B_Block_height', 'C_Block_height']),\n",
    "        (\"reception_loc\", ReceptionTransformer, ['reception_x', 'reception_y']),\n",
    "        (\"set_location_enc\", SetLocationTransformer, ['set_x', 'set_y']),\n",
    "        (\"delta_transformer\", PowerTransformer(standardize=True), ['ScoreDelta']),\n",
    "        (\"passthrough\", \"passthrough\", ['ScoreScaled'])\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "preprocessor_reduced = Pipeline(\n",
    "    [\n",
    "        ('transformer', TransformerReduced),\n",
    "        ('scaler', RobustScaler())\n",
    "    ]\n",
    ")\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TransformerReduced = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"ordinal_enc\",\n",
    "            OrdinalEncoder(categories=[[\"2\", \"3\", \"4\", \"5\", \"6\", \"1\"]]),\n",
    "            [\"rotation_home\"],\n",
    "        ),\n",
    "        (\"call_enc\", SetterCallEncoder, [\"call\"]),\n",
    "        (\n",
    "            \"height_encoder\",\n",
    "            HeightTransformer,\n",
    "            [\"F_Block_height\", \"B_Block_height\", \"C_Block_height\"],\n",
    "        ),\n",
    "        (\"reception_loc\", ReceptionTransformer, [\"reception_x\", \"reception_y\"]),\n",
    "        (\"set_location_enc\", SetLocationTransformer, [\"set_x\", \"set_y\"]),\n",
    "        (\"delta_transformer\", PowerTransformer(standardize=True), [\"ScoreDelta\"]),\n",
    "        (\"passthrough\", \"passthrough\", [\"ScoreScaled\"]),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "preprocessor_reduced = Pipeline(\n",
    "    [(\"transformer\", TransformerReduced), (\"scaler\", RobustScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"models/region_coordinate/no_tuning_reduced/\"\n",
    "if TUNE:\n",
    "    log_nt_reduced_df = helpclass.fit_models(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        preprocessor_reduced,\n",
    "        classifiers,\n",
    "        names,\n",
    "        save_name,\n",
    "        folder,\n",
    "        parameters,\n",
    "        cross_val_splits=5,\n",
    "        verbose=False,\n",
    "    )\n",
    "else:\n",
    "    log_nt_reduced_df = helpclass.eval_models(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifiers,\n",
    "        names,\n",
    "        save_name,\n",
    "        folder,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    log_nt_reduced_df[\n",
    "        [\"model\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"accuracy_train\"]\n",
    "    ].sort_values(by=\"accuracy\", ascending=False)\n",
    ")\n",
    "helpclass.summary_mean_print(log_nt_reduced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Random forest tuning\n",
    "As more data is not available to better tune the algorithm, let's manually tune the random forest hyperparameters to try reducing the over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Effect of number of trees\n",
    "The out-of-bag error is not very affected by the number of trees. A value >200 should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"models/tuning_curves/random_forest/\"\n",
    "\n",
    "RF = RandomForestClassifier(oob_score=True, random_state=42, n_jobs=-1, max_samples=0.5)\n",
    "# RF.set_params(max_features=10, min_samples_split=5)\n",
    "\n",
    "classifier_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor_full), (\"classifier\", RF)]\n",
    ")\n",
    "\n",
    "param = \"n_estimators\"\n",
    "plist = np.geomspace(start=50, stop=1200, dtype=int)\n",
    "\n",
    "if TUNE:\n",
    "    depth_df = helpclass.get_tuning_curve(\n",
    "        classifier_pipe, param, plist, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    depth_df.to_csv(f\"{folder}{param}.csv\")\n",
    "else:\n",
    "    depth_df = pd.read_csv(f\"{folder}{param}.csv\")\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=\"oob_err\",\n",
    "    log_x=True,\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on out-of-bag error\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=[\"train_acc\", \"test_acc\"],\n",
    "    log_x=True,\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on train/test accuracy metric\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Effect of max_features number\n",
    "The default value for classification is max_features = $\\sqrt{\\text{features}} = 4$. As it is a small dataset, it can benefit from a larger number of features available for every tree to avoid over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(oob_score=True, random_state=42, n_jobs=-1, max_samples=0.5)\n",
    "# RF.set_params(max_features=10, min_samples_split=5)\n",
    "\n",
    "classifier_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor_full), (\"classifier\", RF)]\n",
    ")\n",
    "\n",
    "param = \"max_features\"\n",
    "plist = [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "\n",
    "TUNE = False\n",
    "if TUNE:\n",
    "    depth_df = helpclass.get_tuning_curve(\n",
    "        classifier_pipe, param, plist, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    depth_df.to_csv(f\"{folder}{param}.csv\")\n",
    "else:\n",
    "    depth_df = pd.read_csv(f\"{folder}{param}.csv\")\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=\"oob_err\",\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on out-of-bag error\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=[\"train_acc\", \"test_acc\"],\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on train/test accuracy metric\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Effect of number of max_leaf_nodes\n",
    "It is an indirect way to limit the growth of the trees. A lower value would constrain the model more and reduce over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(oob_score=True, random_state=42, n_jobs=-1, max_samples=0.5)\n",
    "\n",
    "classifier_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor_full), (\"classifier\", RF)]\n",
    ")\n",
    "\n",
    "param = \"max_leaf_nodes\"\n",
    "plist = [\n",
    "    5,\n",
    "    8,\n",
    "    10,\n",
    "    12,\n",
    "    15,\n",
    "    18,\n",
    "    20,\n",
    "    22,\n",
    "    25,\n",
    "    27,\n",
    "    30,\n",
    "    33,\n",
    "    35,\n",
    "    38,\n",
    "    40,\n",
    "    43,\n",
    "    45,\n",
    "    48,\n",
    "    50,\n",
    "    55,\n",
    "    60,\n",
    "    65,\n",
    "]\n",
    "TUNE = False\n",
    "if TUNE:\n",
    "    depth_df = helpclass.get_tuning_curve(\n",
    "        classifier_pipe, param, plist, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    depth_df.to_csv(f\"{folder}{param}.csv\")\n",
    "else:\n",
    "    depth_df = pd.read_csv(f\"{folder}{param}.csv\")\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=\"oob_err\",\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on out-of-bag error\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=[\"train_acc\", \"test_acc\"],\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on train/test accuracy metric\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Effect of min_samples_split\n",
    "Larger values decrease over-fitting, by limiting the number of nodes that can be further split based on their population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(oob_score=True, random_state=42, n_jobs=-1, max_samples=0.5)\n",
    "\n",
    "classifier_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor_full), (\"classifier\", RF)]\n",
    ")\n",
    "\n",
    "param = \"min_samples_split\"\n",
    "plist = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "TUNE = False\n",
    "if TUNE:\n",
    "    depth_df = helpclass.get_tuning_curve(\n",
    "        classifier_pipe, param, plist, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    depth_df.to_csv(f\"{folder}{param}.csv\")\n",
    "else:\n",
    "    depth_df = pd.read_csv(f\"{folder}{param}.csv\")\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=\"oob_err\",\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on out-of-bag error\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=[\"train_acc\", \"test_acc\"],\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on train/test accuracy metric\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Final random forest GridSearchCV tuning\n",
    "A grid search among hyperparameters is performed based on the most promising hyperparameters values that reduced the out-of-bag error in the tests above.\n",
    "Compared to the initial no-tune results (Random Forest, accuracy=0.467192, precision=0.470319, recall=0.463167, f1=0.464306, accuracy_train=1.000000) the model scores slightly worse (accuracy = 0.46982, f1=0.459596) but with 0.78 accuracy for the train set (this could be a good sign in terms of reducing over-fitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from helpclass import report_model_performance\n",
    "\n",
    "# putting it all together\n",
    "RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"rf\"\n",
    "name = \"Random Forest\"\n",
    "\n",
    "if TUNE:\n",
    "    params = {\n",
    "        \"classifier__n_estimators\": [170, 250],\n",
    "        \"classifier__max_features\": [5, 10, 13],\n",
    "        # 'classifier__max_depth' : [6, 7, 8],\n",
    "        \"classifier__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"classifier__min_samples_split\": [4, 7, 10],\n",
    "        \"classifier__bootstrap\": [True, False],\n",
    "        \"classifier__max_leaf_nodes\": [30, 43, None],\n",
    "    }\n",
    "\n",
    "    classifier_pipe = Pipeline(\n",
    "        steps=[(\"preprocessor\", preprocessor_full), (\"classifier\", RF)]\n",
    "    )\n",
    "\n",
    "    CV = GridSearchCV(classifier_pipe, params, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "    print(CV.best_params_)\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=params,\n",
    "        verbose=True,\n",
    "    )\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    helpclass.report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Random forest tuning using Hyperopt\n",
    "[Hyperopt](http://hyperopt.github.io/hyperopt/getting-started/overview/) allows for the search of different hyperparameters that minimize a given function (in this case, accuracy). In this case, the results using Hyperopt have been lower than the ones with manual tuning. It is possible that more Hyperopt iterations are needed, or that the model above \"got lucky\" on the test set, but would not perform as well on different test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "space = {\n",
    "    \"max_features\": hp.uniform(\"max_features\", 4, 16),\n",
    "    \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "    \"min_samples_split\": hp.quniform(\"min_samples_split\", 4, 15, 1),\n",
    "    \"max_leaf_nodes\": hp.uniform(\"max_leaf_nodes\", 30, 45),\n",
    "    \"n_estimators\": 200,\n",
    "}\n",
    "\n",
    "from hyperopt import Trials, fmin, tpe\n",
    "\n",
    "X_tr_processed = preprocessor_full.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_full.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_rf(explore_space):\n",
    "    clf = RandomForestClassifier(\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "        warm_start=False,\n",
    "        n_jobs=-1,\n",
    "        max_samples=0.5,\n",
    "        max_features=int(explore_space[\"max_features\"]),\n",
    "        n_estimators=int(explore_space[\"n_estimators\"]),\n",
    "        bootstrap=True,\n",
    "        criterion=explore_space[\"criterion\"],\n",
    "        max_leaf_nodes=int(explore_space[\"max_leaf_nodes\"]),\n",
    "        min_samples_split=int(explore_space[\"min_samples_split\"]),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr_processed, y_train)\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"accuracy\", cv=5)\n",
    "    score_avg = scores.mean()\n",
    "\n",
    "    # print (f\"5-CV mean SCORE: {score_avg}\")\n",
    "    return {\"loss\": -score_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_rf, space=space, algo=tpe.suggest, max_evals=200, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"rf_hyperopt\"\n",
    "name = \"Random Forest\"\n",
    "\n",
    "if TUNE:\n",
    "    # putting it all together\n",
    "    RF = RandomForestClassifier(random_state=42, n_jobs=-1, max_samples=0.5)\n",
    "\n",
    "    params = {\n",
    "        \"classifier__n_estimators\": [200],\n",
    "        \"classifier__max_features\": [11],\n",
    "        \"classifier__criterion\": [\"entropy\"],\n",
    "        \"classifier__min_samples_split\": [8],\n",
    "        \"classifier__bootstrap\": [False],\n",
    "        \"classifier__max_leaf_nodes\": [35],\n",
    "    }\n",
    "\n",
    "    classifier_pipe = Pipeline(\n",
    "        steps=[(\"preprocessor\", preprocessor_full), (\"classifier\", RF)]\n",
    "    )\n",
    "\n",
    "    CV = GridSearchCV(classifier_pipe, params, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "    print(CV.best_params_)\n",
    "    print(CV.scorer_)\n",
    "    helpclass.report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=params,\n",
    "        verbose=True,\n",
    "    )\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Random forest tuned - reduced model\n",
    "With a reduced model (model with reduced numbers of features) the performance loss is very small. The performance of the tuned model is better than the no-tuning alternative (+1.8% accuracy and +1.5% f1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# putting it all together\n",
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"rf_reduced_features\"\n",
    "name = \"Random Forest\"\n",
    "\n",
    "if TUNE:\n",
    "    RF = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "    params = {\n",
    "        \"classifier__n_estimators\": [170, 250],\n",
    "        \"classifier__max_features\": [5, None],\n",
    "        # 'classifier__max_depth' : [6, 7, 8],\n",
    "        \"classifier__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"classifier__min_samples_split\": [4, 7, 10],\n",
    "        \"classifier__bootstrap\": [True, False],\n",
    "        \"classifier__max_leaf_nodes\": [15, 30, 43],\n",
    "    }\n",
    "\n",
    "    classifier_pipe = Pipeline(\n",
    "        steps=[(\"preprocessor\", preprocessor_reduced), (\"classifier\", RF)]\n",
    "    )\n",
    "\n",
    "    CV = GridSearchCV(classifier_pipe, params, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "    print(CV.best_params_)\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=params,\n",
    "        verbose=True,\n",
    "    )\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## XGBoost tuning\n",
    "First, let's evaluate the effect of different parameters on the train and test performance. Later, a search with GridSearchCV will be run to find the hyperparameters that give the best results. Then, Hyperopt will be used and the results compared.\n",
    "\n",
    "XGBoost offers several tuning parameters. As the dataset is small, with high risk of over-fitting, we will need to impose some regularization to have a model that can generalize well to new data. Let's analyze the effect of different hyperparameters on our metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Max_depth\n",
    "Increasing max_depth reduces bias but increases variance. Lower values indicate lower complexity, and they are preferrable to face the over-fitting issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/tuning_curves/xgboost/\"\n",
    "classifier_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor_full),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            xgb.XGBClassifier(\n",
    "                use_label_encoder=False, eval_metric=\"mlogloss\", subsample=0.5\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param = \"max_depth\"\n",
    "plist = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "TUNE = False\n",
    "if TUNE:\n",
    "    depth_df = helpclass.get_tuning_curve(\n",
    "        classifier_pipe, param, plist, X_train, X_test, y_train, y_test, skip_oob=True\n",
    "    )\n",
    "    depth_df.to_csv(f\"{folder}{param}.csv\")\n",
    "else:\n",
    "    depth_df = pd.read_csv(f\"{folder}{param}.csv\")\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=[\"train_acc\", \"test_acc\"],\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on train/test accuracy metric\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### min_child_weight\n",
    "It indirectly impact the complexity of the trees. Lower values allow more node splits to take place, increasing variance and reducing bias, but they may lead to over-fitting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor_full),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            xgb.XGBClassifier(\n",
    "                use_label_encoder=False, eval_metric=\"mlogloss\", subsample=0.5\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param = \"min_child_weight\"\n",
    "plist = np.geomspace(1, 20, num=30)\n",
    "\n",
    "TUNE = False\n",
    "if TUNE:\n",
    "    depth_df = helpclass.get_tuning_curve(\n",
    "        classifier_pipe, param, plist, X_train, X_test, y_train, y_test, skip_oob=True\n",
    "    )\n",
    "    depth_df.to_csv(f\"{folder}{param}.csv\")\n",
    "else:\n",
    "    depth_df = pd.read_csv(f\"{folder}{param}.csv\")\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=[\"train_acc\", \"test_acc\"],\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on train/test accuracy metric\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### gamma\n",
    "Higher values of gamma are more conservative (i.e., they inhibit further partitioning of leaf nodes). Larger values of gamma might reduce over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor_full),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            xgb.XGBClassifier(\n",
    "                use_label_encoder=False, eval_metric=\"mlogloss\", subsample=0.5\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param = \"gamma\"\n",
    "plist = np.geomspace(1e-4, 5, num=30)\n",
    "\n",
    "TUNE = False\n",
    "if TUNE:\n",
    "    depth_df = helpclass.get_tuning_curve(\n",
    "        classifier_pipe, param, plist, X_train, X_test, y_train, y_test, skip_oob=True\n",
    "    )\n",
    "    depth_df.to_csv(f\"{folder}{param}.csv\")\n",
    "else:\n",
    "    depth_df = pd.read_csv(f\"{folder}{param}.csv\")\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(\n",
    "    depth_df,\n",
    "    x=param,\n",
    "    y=[\"train_acc\", \"test_acc\"],\n",
    "    width=800,\n",
    "    title=f\"Effect of {param} on train/test accuracy metric\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### XGBoost manually tuned model\n",
    "- With GridSearchCV, investigate the number of estimators and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Look for number of trees\n",
    "# Following: https://bradleyboehmke.github.io/HOML/gbm.html#xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"xgb\"\n",
    "name = \"XGBoost\"\n",
    "\n",
    "TUNE = False\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__n_estimators\": [100, 180, 300, 500],\n",
    "        \"classifier__eta\": [1e-3, 0.01, 0.05],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                xgb.XGBClassifier(\n",
    "                    use_label_encoder=False, subsample=0.7, eval_metric=\"mlogloss\"\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "    )\n",
    "\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Then, change tree-specific parameters, such as the max_depth and min_child_weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Tree-specific parameters\n",
    "# Following: https://bradleyboehmke.github.io/HOML/gbm.html#xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"xgb\"\n",
    "name = \"XGBoost\"\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__n_estimators\": [300],\n",
    "        \"classifier__eta\": [0.01],\n",
    "        \"classifier__max_depth\": [5, 6, 7],\n",
    "        \"classifier__min_child_weight\": [3, 4, 6, 8],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                xgb.XGBClassifier(\n",
    "                    use_label_encoder=False, subsample=0.7, eval_metric=\"mlogloss\"\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=params,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Introduce the gamma hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 3a. Regularization\n",
    "# Following: https://bradleyboehmke.github.io/HOML/gbm.html#xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"xgb\"\n",
    "name = \"XGBoost\"\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__n_estimators\": [300],\n",
    "        \"classifier__eta\": [0.01],\n",
    "        \"classifier__max_depth\": [6],\n",
    "        \"classifier__min_child_weight\": [3],\n",
    "        \"classifier__gamma\": [0, 0.25, 0.5, 0.75, 1],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                xgb.XGBClassifier(\n",
    "                    use_label_encoder=False, subsample=0.7, eval_metric=\"mlogloss\"\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=params,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Introduce lambda and alpha regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 3b. Regularization\n",
    "# Following: https://bradleyboehmke.github.io/HOML/gbm.html#xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"xgb\"\n",
    "name = \"XGBoost\"\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__n_estimators\": [300],\n",
    "        \"classifier__eta\": [0.01],\n",
    "        \"classifier__max_depth\": [6],\n",
    "        \"classifier__min_child_weight\": [3],\n",
    "        \"classifier__gamma\": [0.5],\n",
    "        \"classifier__lambda\": [1, 1.25, 1.5, 1.75, 2],\n",
    "        \"classifier__alpha\": [0, 0.25, 0.5, 1, 1.5],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                xgb.XGBClassifier(\n",
    "                    use_label_encoder=False, subsample=0.7, eval_metric=\"mlogloss\"\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=params,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Review the learning rate with the new hyperparameters determined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Review learning rate\n",
    "# Following: https://bradleyboehmke.github.io/HOML/gbm.html#xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"xgb\"\n",
    "name = \"XGBoost\"\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__n_estimators\": [300],\n",
    "        \"classifier__eta\": [1e-3, 5e-3, 1e-2, 5e-2, 1e-1],\n",
    "        \"classifier__max_depth\": [6],\n",
    "        \"classifier__min_child_weight\": [3],\n",
    "        \"classifier__gamma\": [0.5],\n",
    "        \"classifier__lambda\": [1],\n",
    "        \"classifier__alpha\": [1],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                xgb.XGBClassifier(\n",
    "                    use_label_encoder=False, subsample=0.8, eval_metric=\"mlogloss\"\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=params,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "# {'classifier__alpha': 0, 'classifier__eta': 0.01, 'classifier__gamma': 1, 'classifier__lambda': 1, 'classifier__max_depth': 6, 'classifier__min_child_weight': 8, 'classifier__n_estimators': 300}\n",
    "# 0.4973684210526315\n",
    "# XGBoost\n",
    "# Accuracy score on test set: 0.46194225721784776\n",
    "# F1-score on test set: 0.45594319100433883"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Now finalize the classifier by tuning it on the complete training set with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"xgb\"\n",
    "name = \"XGBoost\"\n",
    "\n",
    "TUNE = False\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__subsample\": [0.7],\n",
    "        \"classifier__n_estimators\": [300],\n",
    "        \"classifier__max_depth\": [6],\n",
    "        \"classifier__min_child_weight\": [3, 8],\n",
    "        \"classifier__gamma\": [0.5, 1],\n",
    "        \"classifier__eta\": [0.01],\n",
    "        \"classifier__lambda\": [1],\n",
    "        \"classifier__alpha\": [0, 1],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=params,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### XGBoost tuning using hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "space = {\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 4, 8, 1),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0, 3),\n",
    "    \"eta\": hp.uniform(\"eta\", 1e-4, 0.1),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 3),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 1, 4),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 1.5, 10),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 50, 800, 1),\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials, fmin, tpe\n",
    "\n",
    "X_tr_processed = preprocessor_full.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_full.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_xgb(explore_space):\n",
    "    clf = xgb.XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        subsample=0.5,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_estimators=int(explore_space[\"n_estimators\"]),\n",
    "        max_depth=int(explore_space[\"max_depth\"]),\n",
    "        gamma=explore_space[\"gamma\"],\n",
    "        eta=explore_space[\"eta\"],\n",
    "        reg_alpha=explore_space[\"reg_alpha\"],\n",
    "        min_child_weight=explore_space[\"min_child_weight\"],\n",
    "        reg_lambda=explore_space[\"reg_lambda\"],\n",
    "    )\n",
    "\n",
    "    evaluation = [(X_tr_processed, y_train), (X_ts_preprocessed, y_test)]\n",
    "\n",
    "    clf.fit(\n",
    "        X_tr_processed,\n",
    "        y_train,\n",
    "        eval_set=evaluation,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False,\n",
    "    )\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"f1_macro\")\n",
    "    score_avg = scores.mean()\n",
    "\n",
    "    # print (f\"5-CV f1 SCORE: {score_avg}\")\n",
    "    return {\"loss\": -score_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_xgb, space=space, algo=tpe.suggest, max_evals=200, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once a good set of hyperparameters has been found with Hyperopt, retrain a model using those parameters on the full training set with 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"xgb_hyperopt\"\n",
    "name = \"XGBoost\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__subsample\": [0.7],\n",
    "        \"classifier__n_estimators\": [309],\n",
    "        \"classifier__max_depth\": [5],\n",
    "        \"classifier__min_child_weight\": [2.548],\n",
    "        \"classifier__gamma\": [0.1789],\n",
    "        \"classifier__eta\": [0.0158],\n",
    "        \"classifier__lambda\": [1.522],\n",
    "        \"classifier__alpha\": [0.285],\n",
    "        # 'classifier__subsample': [0.7],\n",
    "        # 'classifier__n_estimators': [300],\n",
    "        # 'classifier__max_depth': [6],\n",
    "        # 'classifier__min_child_weight': [4.64],\n",
    "        # 'classifier__gamma': [0.85],\n",
    "        # 'classifier__eta': [0.0106],\n",
    "        # 'classifier__lambda': [2.64],\n",
    "        # 'classifier__alpha': [0.836],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### XGBoost hyperopt reduced model\n",
    "We could repeat the procedure on the reduced model. Results:\n",
    "- XGBoost full model: 0.469 accuracy, 0.463 f1-score\n",
    "- XGBoost reduced model: 0.459 accuracy, 0.458 f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_tr_processed = preprocessor_reduced.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_reduced.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_xgb(explore_space):\n",
    "    clf = xgb.XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        subsample=0.5,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_estimators=int(explore_space[\"n_estimators\"]),\n",
    "        max_depth=int(explore_space[\"max_depth\"]),\n",
    "        gamma=explore_space[\"gamma\"],\n",
    "        eta=explore_space[\"eta\"],\n",
    "        reg_alpha=explore_space[\"reg_alpha\"],\n",
    "        min_child_weight=explore_space[\"min_child_weight\"],\n",
    "        reg_lambda=explore_space[\"reg_lambda\"],\n",
    "    )\n",
    "\n",
    "    evaluation = [(X_tr_processed, y_train), (X_ts_preprocessed, y_test)]\n",
    "\n",
    "    clf.fit(\n",
    "        X_tr_processed,\n",
    "        y_train,\n",
    "        eval_set=evaluation,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        early_stopping_rounds=30,\n",
    "        verbose=False,\n",
    "    )\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"f1_macro\")\n",
    "    scores_avg = scores.mean()\n",
    "\n",
    "    print(f\"5-CV avg SCORE: {scores_avg}\")\n",
    "    return {\"loss\": -scores_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_xgb, space=space, algo=tpe.suggest, max_evals=100, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"xgb_hyperopt_reduced\"\n",
    "name = \"XGBoost\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__subsample\": [0.7],\n",
    "        \"classifier__n_estimators\": [287],\n",
    "        \"classifier__max_depth\": [6],\n",
    "        \"classifier__min_child_weight\": [8.344],\n",
    "        \"classifier__gamma\": [0.837],\n",
    "        \"classifier__eta\": [0.051],\n",
    "        \"classifier__lambda\": [3.093],\n",
    "        \"classifier__alpha\": [0.324],\n",
    "        # 'classifier__subsample': [0.7],\n",
    "        # 'classifier__n_estimators': [300],\n",
    "        # 'classifier__max_depth': [8],\n",
    "        # 'classifier__min_child_weight': [4.96],\n",
    "        # 'classifier__gamma': [0.844],\n",
    "        # 'classifier__eta': [0.029],\n",
    "        # 'classifier__lambda': [2.92],\n",
    "        # 'classifier__alpha': [1.184],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_reduced),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Is it possible to build an efficient and simplified decision tree for every *rotation_home*?\n",
    "Unfortunately, **the simplified decision tree (max depth = 3) accuracy (33%) is lower than the baseline (40%)** for rotation 6 (sample of *rotation_home*). It is unlikely that this method can be used without additional data-points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target = \"region_coordinate\"\n",
    "selected_rotation = \"6\"\n",
    "df_temp6 = df_clean.copy()\n",
    "df_temp6 = df_temp6[df_temp6.rotation_home == selected_rotation]\n",
    "df_temp6.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"Number of rallies for rotation {selected_rotation}: {len(df_temp6)}\")\n",
    "\n",
    "y6 = df_temp6[target]\n",
    "X6 = df_temp6.drop(columns=target)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.5)\n",
    "train_index6, test_index6 = next(sss.split(X6, y6))\n",
    "\n",
    "X_train6 = X6.iloc[train_index6]\n",
    "y_train6 = y6.iloc[train_index6]\n",
    "X_test6 = X6.iloc[test_index6]\n",
    "y_test6 = y6.iloc[test_index6]\n",
    "\n",
    "# Training set\n",
    "enc_target6 = LabelEncoder()\n",
    "print(f\"\\nDistribution in train set:\\n{y_train6.value_counts(normalize=True)}\")\n",
    "print(\n",
    "    f\"\\nBaseline accuracy for rotation {selected_rotation}: {max(y_train6.value_counts(normalize=True))*100:.1f}%\"\n",
    ")\n",
    "y_train6 = enc_target6.fit_transform(y_train6)\n",
    "y_test6 = enc_target6.transform(y_test6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "space = {\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 2, 3, 1),\n",
    "    \"max_features\": hp.uniform(\"max_features\", 2, 8),\n",
    "    # 'max_leaf_nodes' : hp.quniform('max_leaf_nodes', 5, 25, 1),\n",
    "    \"min_samples_split\": hp.quniform(\"min_samples_split\", 5, 10, 1),\n",
    "    # 'min_samples_leaf': hp.quniform(\"min_samples_leaf\", 1, 5, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_tr_processed6 = preprocessor_reduced.fit_transform(X_train6)\n",
    "\n",
    "\n",
    "def objective_dt(explore_space):\n",
    "    clf = DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        max_depth=int(explore_space[\"max_depth\"]),\n",
    "        ccp_alpha=0,\n",
    "        # max_leaf_nodes=int(explore_space['max_leaf_nodes']),\n",
    "        min_samples_split=int(explore_space[\"min_samples_split\"]),\n",
    "        # min_samples_leaf=int(explore_space['min_samples_leaf']),\n",
    "        max_features=int(explore_space[\"max_features\"]),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr_processed6, y_train6)\n",
    "    scores = cross_val_score(clf, X_tr_processed6, y_train6, scoring=\"f1_macro\", cv=5)\n",
    "    scores_avg = scores.mean()\n",
    "\n",
    "    # print (f\"5-CV avg SCORE: {scores_avg}\")\n",
    "    return {\"loss\": -scores_avg, \"status\": STATUS_OK}\n",
    "\n",
    "\n",
    "TUNE = False\n",
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_dt, space=space, algo=tpe.suggest, max_evals=250, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"models/region_coordinate/tuned/\"\n",
    "model = \"dt_hyperopt_reduced\"\n",
    "name = \"Decision Tree\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__ccp_alpha\": [0],\n",
    "        \"classifier__max_depth\": [3],\n",
    "        \"classifier__max_features\": [5],\n",
    "        # 'classifier__max_leaf_nodes': [25],\n",
    "        # 'classifier__min_samples_leaf': [4],\n",
    "        \"classifier__min_samples_split\": [5],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_reduced),\n",
    "            (\"classifier\", DecisionTreeClassifier(random_state=42)),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train6, y_train6)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    report_model_performance(\n",
    "        X_train6,\n",
    "        X_test6,\n",
    "        y_train6,\n",
    "        y_test6,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train6, X_test6, y_train6, y_test6, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if TUNE:\n",
    "    from sklearn import tree\n",
    "\n",
    "    if loaded_model:\n",
    "        dot_data = tree.export_graphviz(\n",
    "            loaded_model.named_steps[\"classifier\"],\n",
    "            out_file=\"images/tree.dot\",\n",
    "            feature_names=get_features_from_transformer_list(\n",
    "                loaded_model.named_steps[\"preprocessor\"].named_steps[\"transformer\"]\n",
    "            ),\n",
    "            class_names=enc_target.classes_,\n",
    "            filled=True,\n",
    "            rounded=True,\n",
    "            special_characters=True,\n",
    "        )\n",
    "        # graph = graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img alt=\"dt\" src=\"images/dt.png\" width=\"900\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusions on *region_combination* target\n",
    "**Using *region_combination* as a target, we were able to achieve 47% accuracy in prediction, up from the 36% baseline**.\n",
    "\n",
    "The result is **encouraging, but not too impressive**. Possible improvements might come from the availability of **more data**, or using **different features** that were not included in this model (more about this in the general conclusions at the end of this work).\n",
    "\n",
    "It is worth noticing that if the process was completely random in the three classifications labels, then it would be impossible to predict the result with more than 1/3 accuracy. The results above show that there is a level of prediction possible, but the mis-classifications could be due to insufficient data, inappropriate features, or also the **random nature of the process**.\n",
    "\n",
    "Next, we will switch to a binary classification problem: we will try to predict whether the setter will set in front of her or behind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "model = load(\"models/region_coordinate/tuned/xgb_hyperopt.joblib\")\n",
    "cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "_, ax = plt.subplots(figsize=(6, 6))\n",
    "ax = sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "labels = [\"L\", \"M\", \"R\"]\n",
    "ax.set_xticklabels(labels, fontsize=20)\n",
    "ax.set_yticklabels(labels, fontsize=20)\n",
    "ax.set_ylabel(\"Prediction\", fontsize=20)\n",
    "ax.set_xlabel(\"Ground Truth\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Target: binary_attacker\n",
    "## Label encoding\n",
    "This label presents a moderate unbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 6. New approach: binarize the label\n",
    "# 1: set in front\n",
    "# 0: set back\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "df_binary = df_clean.copy()\n",
    "\n",
    "target = \"binarized_attacker\"\n",
    "df_binary.reset_index(inplace=True, drop=True)\n",
    "y = df_binary[target]\n",
    "X = df_binary.drop(columns=target)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.25)\n",
    "train_index, test_index = next(sss.split(X, y))\n",
    "\n",
    "X_train = X.iloc[train_index]\n",
    "y_train = y.iloc[train_index]\n",
    "\n",
    "X_test = X.iloc[test_index]\n",
    "y_test = y.iloc[test_index]\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing pipeline\n",
    "This model uses the same preprocessing pipeline as the *region_coordinate* model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## No-tuning results\n",
    "Let's analyze again the performance of out-of-the-box algorithms. It seems that we would be able to predict the setter choice with 69% accuracy (+8% accuracy with respect to the 61% baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Models definition\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "DT = DecisionTreeClassifier(random_state=42)\n",
    "RF = RandomForestClassifier(\n",
    "    oob_score=True, random_state=42, warm_start=True, n_jobs=-1, bootstrap=True\n",
    ")\n",
    "ET = ExtraTreesClassifier(oob_score=True, bootstrap=True, random_state=42, n_jobs=-1)\n",
    "lr = LogisticRegression(\n",
    "    solver=\"saga\", max_iter=2000, tol=1e-2, dual=False, random_state=42\n",
    ")\n",
    "kn = KNeighborsClassifier(n_jobs=-1)\n",
    "lsvc = LinearSVC(random_state=42, tol=1e-2, max_iter=4000, dual=False)\n",
    "svc = SVC(random_state=42, tol=1e-2, max_iter=4000, cache_size=1000)\n",
    "gb = GradientBoostingClassifier(tol=1e-2, random_state=42)\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "xgbm = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "hgb = HistGradientBoostingClassifier(random_state=42, tol=1e-2)\n",
    "\n",
    "classifiers = [DT, RF, ET, lr, kn, lsvc, svc, gb, ada, xgbm, hgb]\n",
    "names = [\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Extra Trees\",\n",
    "    \"Logistic Regression\",\n",
    "    \"K-Neighbors\",\n",
    "    \"Linear SVC\",\n",
    "    \"SVC\",\n",
    "    \"Gradient Boosting\",\n",
    "    \"ADABoost\",\n",
    "    \"XGBoost\",\n",
    "    \"HistGradientBoosting\",\n",
    "]\n",
    "save_name = [\"dt\", \"rf\", \"et\", \"lr\", \"kn\", \"lsvc\", \"svc\", \"gb\", \"ada\", \"xgb\", \"hgb\"]\n",
    "# params = [DT_params, RF_params, ET_params, lr_params, kn_params, lsvc_params, svc_params, gb_params, ada_params, xgb_params]\n",
    "parameters = [{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: FIT TAKES LONG TIME!\n",
    "folder = \"models/binary_attacker/no_tuning/\"\n",
    "\n",
    "if TUNE:\n",
    "    # WARNING: FIT TAKES LONG TIME!\n",
    "    log_binary_df = helpclass.fit_models(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        preprocessor_full,\n",
    "        classifiers,\n",
    "        names,\n",
    "        save_name,\n",
    "        folder,\n",
    "        parameters,\n",
    "        cross_val_splits=5,\n",
    "        verbose=False,\n",
    "    )\n",
    "else:\n",
    "    log_binary_df = helpclass.eval_models(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifiers,\n",
    "        names,\n",
    "        save_name,\n",
    "        folder,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    log_binary_df[\n",
    "        [\"model\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"accuracy_train\"]\n",
    "    ].sort_values(by=\"accuracy\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## No-tuning results (Reduced model)\n",
    "Surprisingly, **the models score slightly better on the model using only a reduced set of features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: FIT TAKES LONG TIME!\n",
    "folder = \"models/binary_attacker/no_tuning_reduced/\"\n",
    "\n",
    "if TUNE:\n",
    "    # WARNING: FIT TAKES LONG TIME!\n",
    "    log_binary_reduced_df = helpclass.fit_models(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        preprocessor_full,\n",
    "        classifiers,\n",
    "        names,\n",
    "        save_name,\n",
    "        folder,\n",
    "        parameters,\n",
    "        cross_val_splits=5,\n",
    "        verbose=False,\n",
    "    )\n",
    "else:\n",
    "    log_binary_reduced_df = helpclass.eval_models(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifiers,\n",
    "        names,\n",
    "        save_name,\n",
    "        folder,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    log_binary_reduced_df[\n",
    "        [\"model\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"accuracy_train\"]\n",
    "    ].sort_values(by=\"accuracy\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## XGBoost binary tuning on full set\n",
    "We will use Hyperopt to explore the hyperparameter space of the XGBoost classifier. The tuned XGBoost algorithm achieves 0.711 accuracy and 0.673 f1-score (+5.2% and +5% with respect to the no-tuning case, respectively.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "space = {\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 1),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 4, 6, 1),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0, 3),\n",
    "    \"eta\": hp.uniform(\"eta\", 1e-4, 0.1),\n",
    "    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 5),\n",
    "    \"reg_lambda\": hp.uniform(\"reg_lambda\", 1, 5),\n",
    "    \"min_child_weight\": hp.uniform(\"min_child_weight\", 1.7, 10),\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_tr_processed = preprocessor_full.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_full.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_xgb(explore_space):\n",
    "    clf = xgb.XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        subsample=0.7,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        n_estimators=int(explore_space[\"n_estimators\"]),\n",
    "        max_depth=int(explore_space[\"max_depth\"]),\n",
    "        gamma=explore_space[\"gamma\"],\n",
    "        eta=explore_space[\"eta\"],\n",
    "        reg_alpha=explore_space[\"reg_alpha\"],\n",
    "        min_child_weight=explore_space[\"min_child_weight\"],\n",
    "        reg_lambda=explore_space[\"reg_lambda\"],\n",
    "    )\n",
    "\n",
    "    evaluation = [(X_tr_processed, y_train), (X_ts_preprocessed, y_test)]\n",
    "\n",
    "    clf.fit(\n",
    "        X_tr_processed,\n",
    "        y_train,\n",
    "        eval_set=evaluation,\n",
    "        eval_metric=\"logloss\",\n",
    "        early_stopping_rounds=30,\n",
    "        verbose=False,\n",
    "    )\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"accuracy\", cv=5)\n",
    "    scores_avg = scores.mean()\n",
    "\n",
    "    # print (f\"5-CV avg SCORE: {scores_avg}\")\n",
    "    return {\"loss\": -scores_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_xgb, space=space, algo=tpe.suggest, max_evals=150, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/binary_attacker/tuned/\"\n",
    "model = \"xgb_hyperopt\"\n",
    "name = \"XGBoost\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__subsample\": [0.7],\n",
    "        \"classifier__n_estimators\": [225],\n",
    "        \"classifier__max_depth\": [6],\n",
    "        \"classifier__min_child_weight\": [3.296],\n",
    "        \"classifier__gamma\": [1.4],\n",
    "        \"classifier__eta\": [0.001],\n",
    "        \"classifier__lambda\": [2.144],\n",
    "        \"classifier__alpha\": [1.495],\n",
    "        #     'classifier__subsample': [0.7],\n",
    "        #     'classifier__n_estimators': [300],\n",
    "        #     'classifier__max_depth': [4],\n",
    "        #     'classifier__min_child_weight': [3.084],\n",
    "        #     'classifier__gamma': [0.657],\n",
    "        #     'classifier__eta': [0.0004],\n",
    "        #     'classifier__lambda': [2.229],\n",
    "        #     'classifier__alpha': [2.720],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### XGBoost binary tuning on reduced features set\n",
    "On the reduced feature set, the tuned XGBoost records 0.709 accuracy and 0.672 f1-score (+2.9% and +2.2% with respect to the untuned model, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_tr_processed = preprocessor_reduced.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_reduced.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_xgb(explore_space):\n",
    "    clf = xgb.XGBClassifier(\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        subsample=0.7,\n",
    "        eval_metric=\"logloss\",\n",
    "        n_estimators=int(explore_space[\"n_estimators\"]),\n",
    "        max_depth=int(explore_space[\"max_depth\"]),\n",
    "        gamma=explore_space[\"gamma\"],\n",
    "        eta=explore_space[\"eta\"],\n",
    "        reg_alpha=explore_space[\"reg_alpha\"],\n",
    "        min_child_weight=explore_space[\"min_child_weight\"],\n",
    "        reg_lambda=explore_space[\"reg_lambda\"],\n",
    "    )\n",
    "\n",
    "    evaluation = [(X_tr_processed, y_train), (X_ts_preprocessed, y_test)]\n",
    "\n",
    "    clf.fit(\n",
    "        X_tr_processed,\n",
    "        y_train,\n",
    "        eval_set=evaluation,\n",
    "        eval_metric=\"logloss\",\n",
    "        early_stopping_rounds=30,\n",
    "        verbose=False,\n",
    "    )\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"accuracy\", cv=5)\n",
    "    accuracy_avg = scores.mean()\n",
    "\n",
    "    print(f\"5-CV Accuracy SCORE: {accuracy_avg}\")\n",
    "    return {\"loss\": -accuracy_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_xgb, space=space, algo=tpe.suggest, max_evals=150, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "folder = \"models/binary_attacker/tuned/\"\n",
    "model = \"xgb_hyperopt_reduced\"\n",
    "name = \"XGBoost\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__subsample\": [0.7],\n",
    "        \"classifier__n_estimators\": [131],\n",
    "        \"classifier__max_depth\": [4],\n",
    "        \"classifier__min_child_weight\": [3.7],\n",
    "        \"classifier__gamma\": [0.008],\n",
    "        \"classifier__eta\": [0.0012],\n",
    "        \"classifier__lambda\": [4.36],\n",
    "        \"classifier__alpha\": [0.11],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_reduced),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SVC tuning on full set\n",
    "Let's try tuning other algorithm to achieve better performance, and possibly stacking multiple classifiers.\n",
    "\n",
    "Exploring the hyperspace to maximize the f1-score, we obtained an accuracy of 0.654 (+2.7%) and an f1-score of (+6.5%). The **performance on the test set is still lower than for the XGBoost classifier**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "space = {\n",
    "    \"C\": hp.loguniform(\"C\", -2, 1),\n",
    "    \"gamma\": hp.loguniform(\"gamma\", -2, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_tr_processed = preprocessor_full.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_full.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_svc(explore_space):\n",
    "    clf = SVC(\n",
    "        random_state=42,\n",
    "        cache_size=1000,\n",
    "        tol=1e-3,\n",
    "        kernel=\"rbf\",\n",
    "        probability=True,\n",
    "        C=float(explore_space[\"C\"]),\n",
    "        gamma=float(explore_space[\"gamma\"]),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr_processed, y_train)\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"f1\", cv=5)\n",
    "    accuracy_avg = scores.mean()\n",
    "\n",
    "    print(f\"5-CV Accuracy SCORE: {accuracy_avg:.4f}\")\n",
    "    return {\"loss\": -accuracy_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_svc, space=space, algo=tpe.suggest, max_evals=200, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)\n",
    "    print(f\"Loss max: {min(trials.losses())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"models/binary_attacker/tuned/\"\n",
    "model = \"svc_hyperopt\"\n",
    "name = \"SVC\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__C\": [2.6675],\n",
    "        \"classifier__class_weight\": [None],\n",
    "        \"classifier__gamma\": [0.146],\n",
    "        \"classifier__kernel\": [\"rbf\"],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                SVC(random_state=42, cache_size=1000, tol=1e-2, probability=True),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SVC on reduced set\n",
    "For the reduced set, the optimization of the f1-score led to a +0.2% in accuracy and +6.8% in f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_tr_processed = preprocessor_reduced.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_reduced.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_svc(explore_space):\n",
    "    clf = SVC(\n",
    "        random_state=42,\n",
    "        cache_size=1000,\n",
    "        tol=5e-3,\n",
    "        kernel=\"rbf\",\n",
    "        C=float(explore_space[\"C\"]),\n",
    "        gamma=float(explore_space[\"gamma\"]),\n",
    "        probability=True,\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr_processed, y_train)\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"f1\", cv=5)\n",
    "    accuracy_avg = scores.mean()\n",
    "\n",
    "    # print (f\"5-CV Accuracy SCORE: {accuracy_avg:.4f}\")\n",
    "    return {\"loss\": -accuracy_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_svc, space=space, algo=tpe.suggest, max_evals=300, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)\n",
    "    print(f\"Loss max: {min(trials.losses())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"models/binary_attacker/tuned/\"\n",
    "model = \"svc_hyperopt_reduced\"\n",
    "name = \"SVC\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__C\": [2.70],\n",
    "        \"classifier__class_weight\": [None],\n",
    "        \"classifier__gamma\": [0.343],\n",
    "        \"classifier__kernel\": [\"rbf\"],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_reduced),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                SVC(random_state=42, cache_size=1000, tol=5e-2, probability=True),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    helpclass.report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logistic regression tuning on full set\n",
    "In this case, tuning did not lead to any improvement (about -1.5% for both accuracy and f1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "\n",
    "space = {\n",
    "    \"C\": hp.loguniform(\"C\", -2, 0),\n",
    "}\n",
    "\n",
    "X_tr_processed = preprocessor_full.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_full.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_logit(explore_space):\n",
    "    clf = LogisticRegression(\n",
    "        random_state=42,\n",
    "        max_iter=4000,\n",
    "        tol=1e-3,\n",
    "        dual=False,\n",
    "        solver=\"liblinear\",\n",
    "        penalty=\"l2\",\n",
    "        C=float(explore_space[\"C\"]),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr_processed, y_train)\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"f1\", cv=5)\n",
    "    accuracy_avg = scores.mean()\n",
    "\n",
    "    # print (f\"5-CV Accuracy SCORE: {accuracy_avg:.4f}\")\n",
    "    return {\"loss\": -accuracy_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_logit, space=space, algo=tpe.suggest, max_evals=300, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)\n",
    "    print(f\"Loss max: {min(trials.losses())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"models/binary_attacker/tuned/\"\n",
    "model = \"lr_hyperopt\"\n",
    "name = \"Logistic Regression\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__C\": [0.679],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                LogisticRegression(\n",
    "                    random_state=42,\n",
    "                    max_iter=2000,\n",
    "                    tol=1e-3,\n",
    "                    dual=False,\n",
    "                    solver=\"liblinear\",\n",
    "                    penalty=\"l2\",\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    helpclass.report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Logistic regression on reduced set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "\n",
    "space = {\n",
    "    \"C\": hp.loguniform(\"C\", -3, 0),\n",
    "    \"l1_ratio\": hp.uniform(\"l1_ratio\", 0, 1),\n",
    "}\n",
    "\n",
    "X_tr_processed = preprocessor_reduced.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_reduced.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_logit(explore_space):\n",
    "    clf = LogisticRegression(\n",
    "        random_state=42,\n",
    "        max_iter=2000,\n",
    "        tol=1e-3,\n",
    "        dual=False,\n",
    "        solver=\"saga\",\n",
    "        penalty=\"elasticnet\",\n",
    "        n_jobs=-1,\n",
    "        C=float(explore_space[\"C\"]),\n",
    "        l1_ratio=float(explore_space[\"l1_ratio\"]),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr_processed, y_train)\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"f1\", cv=5)\n",
    "    accuracy_avg = scores.mean()\n",
    "\n",
    "    # print (f\"5-CV Accuracy SCORE: {accuracy_avg:.4f}\")\n",
    "    return {\"loss\": -accuracy_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_logit, space=space, algo=tpe.suggest, max_evals=400, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)\n",
    "    print(f\"Loss max: {min(trials.losses())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"models/binary_attacker/tuned/\"\n",
    "model = \"lr_hyperopt_reduced\"\n",
    "name = \"Logistic Regression\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__C\": [0.706],\n",
    "        \"classifier__l1_ratio\": [0.16],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                LogisticRegression(\n",
    "                    random_state=42,\n",
    "                    max_iter=2000,\n",
    "                    tol=5e-3,\n",
    "                    dual=False,\n",
    "                    solver=\"saga\",\n",
    "                    penalty=\"elasticnet\",\n",
    "                    n_jobs=-1,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    helpclass.report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## K-Neighbors tuning on full set\n",
    "Tuning leads to a +4.2% accuracy and +4% f1-score. However, K-Neighbors (as Logistic Regression) in this case led only to marginal improvements with respect to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "space = {\n",
    "    \"n_neighbors\": hp.uniform(\"n_neighbors\", 5, 40),\n",
    "    \"weights\": hp.choice(\"weights\", [\"uniform\", \"distance\"]),\n",
    "    \"leaf_size\": hp.uniform(\"leaf_size\", 10, 40),\n",
    "}\n",
    "\n",
    "\n",
    "X_tr_processed = preprocessor_full.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_full.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_knn(explore_space):\n",
    "    clf = KNeighborsClassifier(\n",
    "        n_jobs=-1,\n",
    "        n_neighbors=int(explore_space[\"n_neighbors\"]),\n",
    "        weights=explore_space[\"weights\"],\n",
    "        leaf_size=int(explore_space[\"leaf_size\"]),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr_processed, y_train)\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"f1\", cv=5)\n",
    "    accuracy_avg = scores.mean()\n",
    "\n",
    "    # print (f\"5-CV Accuracy SCORE: {accuracy_avg:.4f}\")\n",
    "    return {\"loss\": -accuracy_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_knn, space=space, algo=tpe.suggest, max_evals=300, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)\n",
    "    print(f\"Loss max: {min(trials.losses())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"models/binary_attacker/tuned/\"\n",
    "model = \"knn_hyperopt\"\n",
    "name = \"KNN\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__leaf_size\": [34],\n",
    "        \"classifier__n_neighbors\": [7],\n",
    "        \"classifier__weights\": [\"distance\"],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_full),\n",
    "            (\"classifier\", KNeighborsClassifier(n_jobs=-1)),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### K-Neighbors on reduced set\n",
    "After tuning: +7% accuracy and +6.7% f1-score (better than the full model!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import STATUS_OK, Trials, fmin, tpe\n",
    "\n",
    "space = {\n",
    "    \"n_neighbors\": hp.uniform(\"n_neighbors\", 5, 40),\n",
    "    \"weights\": hp.choice(\"weights\", [\"uniform\", \"distance\"]),\n",
    "    \"leaf_size\": hp.uniform(\"leaf_size\", 10, 40),\n",
    "}\n",
    "\n",
    "\n",
    "X_tr_processed = preprocessor_reduced.fit_transform(X_train)\n",
    "X_ts_preprocessed = preprocessor_reduced.transform(X_test)\n",
    "\n",
    "\n",
    "def objective_knn(explore_space):\n",
    "    clf = KNeighborsClassifier(\n",
    "        n_jobs=-1,\n",
    "        n_neighbors=int(explore_space[\"n_neighbors\"]),\n",
    "        weights=explore_space[\"weights\"],\n",
    "        leaf_size=int(explore_space[\"leaf_size\"]),\n",
    "    )\n",
    "\n",
    "    clf.fit(X_tr_processed, y_train)\n",
    "    scores = cross_val_score(clf, X_tr_processed, y_train, scoring=\"f1\", cv=5)\n",
    "    accuracy_avg = scores.mean()\n",
    "\n",
    "    # print (f\"5-CV Accuracy SCORE: {accuracy_avg:.4f}\")\n",
    "    return {\"loss\": -accuracy_avg, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if TUNE:\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(\n",
    "        fn=objective_knn, space=space, algo=tpe.suggest, max_evals=300, trials=trials\n",
    "    )\n",
    "    print(\"The best hyperparameters are : \", \"\\n\")\n",
    "    print(best_hyperparams)\n",
    "    print(f\"Loss max: {min(trials.losses())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"models/binary_attacker/tuned/\"\n",
    "model = \"knn_hyperopt_reduced\"\n",
    "name = \"KNN\"\n",
    "\n",
    "if TUNE:\n",
    "    param_grid = {\n",
    "        \"classifier__leaf_size\": [15],\n",
    "        \"classifier__n_neighbors\": [25],\n",
    "        \"classifier__weights\": [\"distance\"],\n",
    "    }\n",
    "\n",
    "    complete_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor_reduced),\n",
    "            (\"classifier\", KNeighborsClassifier(n_jobs=-1)),\n",
    "        ]\n",
    "    )\n",
    "    CV = GridSearchCV(complete_pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    CV.fit(X_train, y_train)\n",
    "\n",
    "    print(CV.best_params_)\n",
    "    print(CV.best_score_)\n",
    "    dump(CV.best_estimator_, f\"{folder}{model}.joblib\")\n",
    "    helpclass.report_model_performance(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        CV.best_estimator_,\n",
    "        name,\n",
    "        param_grid=param_grid,\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    loaded_model = load(f\"{folder}{model}.joblib\")\n",
    "    print(loaded_model.named_steps[\"classifier\"])\n",
    "    print(\"\\n\")\n",
    "    report_model_performance(\n",
    "        X_train, X_test, y_train, y_test, loaded_model, name, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ensemble results\n",
    "In this case, **performance with an ensemble (XGBoost + KNN + SVC, on reduced set, with hard voting) is lower than the performance of the single XGBoost Classifier**.\n",
    "\n",
    "<code>\n",
    "estimators = [\n",
    "    (\"xgb\", xgbc.named_steps[\"classifier\"]),\n",
    "    (\"knn\", knn.named_steps[\"classifier\"]),\n",
    "    (\"svc\", svc.named_steps[\"classifier\"])\n",
    "]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessing\", preprocessor_reduced),\n",
    "    (\"classifier\", VotingClassifier(estimators, voting=\"hard\"))\n",
    "])\n",
    "</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "svc = load(\"models/binary_attacker/tuned/svc_hyperopt_reduced.joblib\")\n",
    "knn = load(\"models/binary_attacker/tuned/knn_hyperopt_reduced.joblib\")\n",
    "xgbc = load(\"models/binary_attacker/tuned/xgb_hyperopt_reduced.joblib\")\n",
    "name = \"ensamble\"\n",
    "\n",
    "estimators = [\n",
    "    (\"xgb\", xgbc.named_steps[\"classifier\"]),\n",
    "    (\"knn\", knn.named_steps[\"classifier\"]),\n",
    "    (\"svc\", svc.named_steps[\"classifier\"]),\n",
    "]\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessor_reduced),\n",
    "        (\"classifier\", VotingClassifier(estimators, voting=\"hard\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mod = pipe.fit(X_train, y_train)\n",
    "helpclass.report_model_performance(\n",
    "    X_train, X_test, y_train, y_test, mod, name, param_grid={}, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusions on *binary_attacker* target\n",
    "Once again the results are positive, but still not quite accurate. The classifier (XGBoost) presents a lot of \"false-negatives\" (predicts Back-0 while the ground truth is Front-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "model = load(\"models/binary_attacker/tuned/xgb_hyperopt.joblib\")\n",
    "cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "_, ax = plt.subplots(figsize=(6, 6))\n",
    "ax = sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "labels = [\"Back\", \"Front\"]\n",
    "ax.set_xticklabels(labels, fontsize=20)\n",
    "ax.set_yticklabels(labels, fontsize=20)\n",
    "ax.set_ylabel(\"Prediction\", fontsize=20)\n",
    "ax.set_xlabel(\"Ground Truth\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model interpretation\n",
    "The best models I found for this dataset are tree-based (like XGBoost and Random Forest). With complex models, it is not always clear which features are contributing to a prediction. For that, I will use the [SHAP](https://github.com/slundberg/shap) (SHapley Additive exPlanations) package, based on the Shapley concept, which show the impact of each feature into the model prediction. It's important to highlight that this analysis is referred only to the model prediction, and not the ground truth.\n",
    "\n",
    "I will use the XGBoost reduced model developed for the *region_coordinate* target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "model = load(\"models/region_coordinate/tuned/xgb_hyperopt.joblib\")\n",
    "\n",
    "features = get_features_from_transformer_list(\n",
    "    model.named_steps[\"preprocessor\"].named_steps[\"transformer\"]\n",
    ")\n",
    "\n",
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "explainer = shap.TreeExplainer(model[\"classifier\"])\n",
    "test_X = pd.DataFrame(data=model[\"preprocessor\"].transform(X_test), columns=features)\n",
    "shap_values = explainer.shap_values(test_X)\n",
    "categories = pd.Series(enc_target.classes_)\n",
    "print(f\"Targets:\\n{categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Global interpretability\n",
    "### Left (L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at the importance of different features when the setter set LEFT (L, encoded as class 0).\n",
    "\n",
    "- ***set_x* seems to have the largest impact on the prediction**. With large values of *set_x* (in pink in the plot, correspond to the setter moving to the R side of the court), the model tend to classify the L output class as less likely. This means that **the model learned that as the setter moves to the right side of the court, she tends to NOT set the ball to the left side**. **The model thinks she is a setter that plays the short distance**.\n",
    "- *F_Block_height*: the taller the block of the front player (setter or opposite), the less the model thinks she will set to the L-side attacker.\n",
    "- *B_Block_height*: the taller the block of the back player (outside hitters), the more the model thinks she will set to the L-side attacker.\n",
    "- *call*: for calls behind the setter (i.e., K2 or KF, large values of *call* feature) the model thinks the play with the L-player is less likely, while it is more likely for calls K7 (low value of *call*).\n",
    "- *rotation_home*: large values (i.e., three-attackers rotations) the L-attacker is less likely than in two-attackers rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(\n",
    "    shap_values[0],\n",
    "    test_X,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Right (R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the R-attacker (encoded as class 2):\n",
    "- *rotation_home*: the feature with most impact. Very high value of *rotation_home* (rotation 1 according to the encoding) indicate much less probability of serving the R side, while intermediate values (rotation 6, 5) greatly increase its probability.\n",
    "- *call*: Large values of call (Kf and K2 especially) are associated with very large probability of attacking the R side of the net.\n",
    "- *set_y*: increasing *set_y* (setter location closer and closer to the net) is associated with lower R-side probability and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[2], test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Middle (M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the M-side attacks (encoded as class 1):\n",
    "- *rotation_home*: the feature with most impact. Very high value of *rotation_home* (rotation 1 mostly, according to the encoding, and mildly for rotation 6) indicate much higher probability of serving the M-attacker. Rotation 5 is associated with less probability.\n",
    "- *B_Block_height*: **Small values of *B_Block_height* are associated with large probability of serving the M-attacker**.\n",
    "- *set_y*: increasing *set_y* (setter location closer and closer to the net) is associated with higher M-attacker probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Partial dependence plots\n",
    "### Importance of *set_x* in L/R decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can look at the partial dependence plot, to show the relationship of one or two variables on the predicted outcome.\n",
    "\n",
    "For example, looking at the effect of *set_x* on the **L-side of the net** Shapley values, as the setter moves to the R side (increasing *set_x* on the x-axis) the lower its Shapley value. The model thinks that when the setter moves to the R-side, she will not set to the L-side of the net (**setter plays the short distance**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"set_x\", shap_values[0], X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Conversely, when looking at the **R-side** of the net, as the setter moves right, a play with the R-side is more likely. **As she moves left and past the left-half of the court, she will not play with the R-side, and this is especially true if the F-blocker is short** (blue dots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"set_x\", shap_values[2], X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Importance of *set_y* in M decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the central area of the net (M), attacking the M-area becomes increasingly more likely as the setter moves closer to the net (larger *set_y*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"set_y\", shap_values[1], X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the C-area of the court, with call K7 it is unlikely for the C-area to be served, especially if *reception_x* is small (reception on the R-side of the court, zone 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"call\", shap_values[1], X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Local predictability: single side-out prediction\n",
    "It is also possible to **explain the model prediction for a single side-out event**. In red, the factors that increase the estimation *f(x)* with respect to the *base_value*, in blue the factors that decrease the estimated probability. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "example_index = 25\n",
    "display(X_test[features].iloc[[example_index]])\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "R-Side estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    explainer.expected_value[0],\n",
    "    shap_values[0][example_index, :],\n",
    "    X_test[features].iloc[example_index, :],\n",
    "    link=\"logit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "M-Side estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    explainer.expected_value[1],\n",
    "    shap_values[1][example_index, :],\n",
    "    X_test[features].iloc[example_index, :],\n",
    "    link=\"logit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "L-Side estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    explainer.expected_value[2],\n",
    "    shap_values[2][example_index, :],\n",
    "    X_test[features].iloc[example_index, :],\n",
    "    link=\"logit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Which side of the net got attacked in the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Side attacked: {pd.Series(enc_target.inverse_transform(y_test)).iloc[example_index]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this case I was \"lucky\" with the prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# General conclusions\n",
    "The model delivered **reasonable performance above the baseline, but not at a satisfactory levels** (80% accuracy on the *region_combination* model would be considered excellent and dependable). The issues can be summarized as:\n",
    "- **Not enough data-points** to develop the models\n",
    "- Possibly missing some important features that can better help to classify the instances\n",
    "- The **process (setter choosing which area to attack) might be at least partially aleatory** in nature\n",
    "- The **idea of simple decision trees for players does not seem ready to work** (yet?)\n",
    "- **Gradient boosting models (like XGBoost) delivered the best performance, followed by RandomForest**\n",
    "\n",
    "Some general conclusion on the setter behavior could be extracted nevertheless:\n",
    "- importance of location of the court (*set_x*) in attacking L/R side (**setter plays the short distance**)\n",
    "- **the setter seems to prefer to attack the short block**. This is often one of the most important factor in her decision\n",
    "- **very different choices in rotation 1** (prefer to attack the center of the court)\n",
    "- **importance of the distance from the net (*set_y*) in inhibiting the attack of the center of the court (M)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Suggestions and next steps\n",
    "To solve the problems above, a few idea come to mind:\n",
    "- Use a different training/test split, increasing the dimension of the test split to reduce the variance of the estimated performance. This must be paired with the need of bootstrapping during the construction of the model\n",
    "- **Use other datasets similar in nature for the training of the models** (this could be much more useful with a **neural network**, allowing the re-use of some trained layers)\n",
    "- The use of other boosting models (LightGBM, Catboost) does not seem to be the answer to this problems\n",
    "- **Introduce new features** that summarize the \"history\" that brought to the current instance. For example:\n",
    "    1. Players attack past performance in the current set or match\n",
    "    2. Players number of attacks in the current set or match\n",
    "    3. Previous set choice in the last side-out rally\n",
    "    4. Opponent setter set choice in the last side-out rally (to identify \"mirror\" setters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "492px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}